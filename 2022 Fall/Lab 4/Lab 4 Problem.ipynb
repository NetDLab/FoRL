{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# EN.520.637 Foundations of Reinforcement Learning\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Lab 4: Multi-armed Bandit and Monte Carlo Method  </font></h1>\n",
    "\n",
    "\n",
    "\n",
    "## Deadline\n",
    "11:59 pm Nov 5th, 2021 \n",
    "\n",
    "##  Content\n",
    "1. Multi-armed Bandit\n",
    "2. Monte Carlo Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement [P 21, Ch 2.3, Sutton]\n",
    "\n",
    "#### Define a 10-armed bandit problem in which the action values $q_*(a)$, $a = 1,...,10$, are samples from a standard norm distribution, i.e. Gaussian distribution with mean $= 0$ and variance $ = 1$. Then, when selected $A_t$ at time step $t$, the actual reward, $R_t$ is selected from a Gaussian distribution with mean = $q_*(A_t)$ and variance = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1.  Greedy and $\\epsilon$-greedy algorithm\n",
    "\n",
    "1. Implement a function/functions that run this game 2000 times with $\\epsilon$-greedy algorithm. Your function/functions should take $\\epsilon$ as one of the inputs and output:\n",
    "<br>   (a) average reward at each time step\n",
    "<br>   (b) percentage of optimal action at each time step. (optimal action is defined by $a^* = arg\\underset{a}max           \\, q^*(a)$ )\n",
    "2. Call your function/functions to generate the average reward and percentage of optimal action at each time step with:\n",
    "<br>   (a) Greedy-algorithm \n",
    "<br>   (b) $\\epsilon$-greedy algorithm, $\\epsilon=0.01$ \n",
    "<br>   (c) $\\epsilon$-greedy algorithm, $\\epsilon=0.1$. \n",
    "3. Plot the average reward and percentage of optimal action of those three cases and compare with [P 23 Fig 2.2 Sutton]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2.  UCB Action Selection\n",
    "\n",
    "1. Implement a function/functions that run this game 2000 times with UCB Action Selection algorithm. Your function/functions should take $c$ as one of the inputs and output:\n",
    "<br>   - average reward at each time step.\n",
    "2. Call your function/functions to generate the average reward at each time step with:\n",
    "<br>   - UCB Action Selection algorithm, $c = 2$.\n",
    "3. Plot the average reward of 2.2 and 1.2c, then compare with [P 28 Fig 2.4 Sutton]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo Method  (CartPole-v1 environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 CartPole Introduction\n",
    "\n",
    "We now apply Monte Carlo Method for CartPole problem. \n",
    "\n",
    "\n",
    "1. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "\n",
    "0. The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "\n",
    "0. The pendulum starts upright, and the goal is to prevent it from falling over. \n",
    "\n",
    "0. A reward of +1 is provided for every timestep that the pole remains upright. \n",
    "\n",
    "0. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "0. For more info (See [SOURCE ON GITHUB](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)).\n",
    "\n",
    "The following examples show the basic usage of this testing environment: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Episode initialization and Initial Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset() ##Initial an episode\n",
    "\n",
    "print(\"Inital observation is {}\".format(observation))\n",
    "\n",
    "print(\"\\nThis means the cart current position is {}\".format(observation[0]), end = '')\n",
    "print(\" with velocity {},\".format(observation[1]))\n",
    "\n",
    "print(\"and the pole current angular position is {}\".format(observation[2]), end = '')\n",
    "print(\" with angular velocity {},\".format(observation[3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Take actions\n",
    "\n",
    "\n",
    "Use env.step(action) to take an action\n",
    "\n",
    "action is an integer from 0 to 1\n",
    "\n",
    "0: \"Left\"; 1: \"Right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current observation is {}\".format(observation))\n",
    "\n",
    "action = 0 #go left\n",
    "observation, reward, done, info = env.step(action) # simulate one step\n",
    "\n",
    "print(\"\\nNew observation is {}\".format(observation))\n",
    "print(\"Step reward is {}\".format(reward))\n",
    "print(\"Did episode just ends? -{}\".format(done)) # episode ends when 3.1(6) happens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Simulate multiple episodes\n",
    "\n",
    "(You may uncomment those lines to see an animation. However, it will not work for JupyterHub since the animation requires GL instead of webGL. If you have Jupyter notebook localy on your computer, this version of code will work through a virtual frame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "total_reward = 0\n",
    "ep_num = 0\n",
    "# img = plt.imshow(env.render(mode='rgb_array')) \n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    #     img.set_data(env.render(mode='rgb_array')) \n",
    "    #     display.display(plt.gcf())\n",
    "    #     display.clear_output(wait=True)\n",
    "    \n",
    "    action = env.action_space.sample()     # this takes random actions\n",
    "    observation, reward, done, info = env.step(action) \n",
    "       \n",
    "    total_reward += reward\n",
    "    \n",
    "\n",
    "\n",
    "    if done:                               # episode just ends\n",
    "        observation = env.reset()          # reset episode\n",
    "        ep_num += 1\n",
    "\n",
    "print(\"Average reward per episode is {}\".format(total_reward/ep_num))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 States Discretization \n",
    "\n",
    "The class DiscreteObs() discretizes the observation space into discrete state space, based on numpy.digitize (Please read its description in https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) \n",
    "\n",
    "Discretization of observation space is necessary for tabular methods. You can use DiscreteObs() or any other library for discretizing the observation space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretObs():\n",
    "    \n",
    "    \n",
    "    def __init__(self, bins_list):\n",
    "        self._bins_list = bins_list\n",
    "        \n",
    "        self._bins_num = len(bins_list)\n",
    "        self._state_num_list = [len(bins)+1 for bins in bins_list]\n",
    "        self._state_num_total = np.prod(self._state_num_list)\n",
    "    \n",
    "    def get_state_num_total(self):\n",
    "        \n",
    "        return self._state_num_total\n",
    "    \n",
    "    def obs2state(self, obs):\n",
    "        \n",
    "        if not len(obs)==self._bins_num:\n",
    "            raise ValueError(\"observation must have length {}\".format(self._bins_num))\n",
    "        else:\n",
    "            return [np.digitize(obs[i], bins=self._bins_list[i]) for i in range(self._bins_num)]\n",
    "        \n",
    "    def obs2idx(self, obs):\n",
    "        \n",
    "        state = self.obs2state(obs)\n",
    "        \n",
    "        return self.state2idx(state)\n",
    "    \n",
    "    def state2idx(self, state):\n",
    "        \n",
    "        idx = 0\n",
    "        for i in range(self._bins_num-1,-1,-1):\n",
    "            idx = idx*self._state_num_list[i]+state[i]\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "    def idx2state(self, idx):\n",
    "        \n",
    "        state = [None]*self._bins_num\n",
    "        state_num_cumul = np.cumprod(self._state_num_list)\n",
    "        for i in range(self._bins_num-1,0,-1):\n",
    "            state[i] = idx/state_num_cumul[i-1]\n",
    "            idx -=state[i]*state_num_cumul[i-1]\n",
    "        state[0] = idx%state_num_cumul[0]\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Recommended Discretization for Carpole-v1 when using Monte-Carlo methods\n",
    "bins_pos = np.linspace(-2.4,2.4,40)        # position\n",
    "bins_d_pos = np.linspace(-3,3,5)           # velocity\n",
    "bins_ang = np.linspace(-0.2618,0.2618,40)  # angle\n",
    "bins_d_ang = np.linspace(-0.3,0.3,5)       # angular velocity\n",
    "\n",
    "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
    "observation = env.reset()\n",
    "\n",
    "state = dobs.obs2state(observation)\n",
    "idx = dobs.obs2idx(observation)\n",
    "\n",
    "print(\"Current position of the cart is {:.4f}\\n\".format(observation[0]))\n",
    "print(\"Current velocity of the cart is {:.4f}\\n\".format(observation[1]))\n",
    "print(\"Current angular position of the pole is {:.4f} rad\\n\".format(observation[2]))\n",
    "print(\"Current angular velocity of the pole is {:.4f} rad\\n\".format(observation[3]))\n",
    "\n",
    "print(\"which are mapped to state {}, with corresponding index {}\".format(state,idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 On-policy first-visit MC control\n",
    "\n",
    "1. Implement \"On-policy first-visit MC control\" algorithum in [Ch 5.4 Sutton] to choose optimal actions\n",
    "2. Simulate this algorithum for 40000 episodes.\n",
    "3. Divide the previous 40000 episodes into 20 sets. Plot average rewards for each sets. (i.e. plot average rewards for the first 2000 episodes, the second 2000 episodes, ..., and the 15th 2000 episodes.) \n",
    "4. Use greedy policy of the trained Q function to control the carpole for 100 episode, plot the accumulate rewards over 100 episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested functions (Feel free to modify existing and add new functions)\n",
    "\n",
    "def get_action(current_state, Q, epsilon):\n",
    "    \n",
    "    # Choose optimal action based on current state and Q\n",
    "    #\n",
    "    # input:  current state,  (array) \n",
    "    #         Q,              (array)  \n",
    "    #         epsilonn,       (float)  \n",
    "    # output: action\n",
    "    #         \n",
    "    return action\n",
    "\n",
    "\n",
    "\n",
    "def update_Q(Q, observation_list, action_list):\n",
    "    # Update Q at the end of each episode\n",
    "    #\n",
    "    # input:  current Q, (array) \n",
    "    #         observation_list,       (array)  states observed in this episode\n",
    "    #         action_list,       (array)  actions took in this spisode\n",
    "    # output: Updated Q\n",
    "    #         \n",
    "\n",
    "        \n",
    "    return Q\n",
    "\n",
    "\n",
    "## Suggested flow (Feel free to modify and add)\n",
    "\n",
    "# parameters for epsilon-greedy algorithm, when epsilon_decay_rate=1, the algorothm implement a fixed \n",
    "# epsilon value as epsilon_start, you can choose either fixed epsilon or decaying epsilon\n",
    "\n",
    "# epsilon_start = 0.3\n",
    "# epsilon_decay_rate = 0.97\n",
    "\n",
    "set_num = 30\n",
    "s = 0\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "\n",
    "#epsilon = epsilon_start   # set epsilon\n",
    "\n",
    "while 1:\n",
    "    \n",
    "    \n",
    "    current_state =                             # discretize the observation space\n",
    "    \n",
    "    action = get_action(current_state,Q,epsilon)# pick action by epsilon greedy policy\n",
    "    \n",
    "    observation, reward, done, info = env.step(action) # simulate one step\n",
    "    \n",
    "    if done:  # end of epsode\n",
    "        Q = update_Q(Q, observation_list, action_list) # update Q for past observations in the episode\n",
    "        \n",
    "        ep_num += 1\n",
    "        \n",
    "        if  np.mod(ep_num,2000)==0: # end of every set of episode\n",
    "            \n",
    "            #epsilon = epsilon*epsilon_decay_rate     # update epsilon\n",
    "            s+=1\n",
    "            \n",
    "            if s == set_num:\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your result here (Feel free to modify)\n",
    "# the result_mc should be a (set_num, )-numpy array that records the average reward of a set of episodes \n",
    "\n",
    "# put your result here\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "figure(figsize=(12,6))\n",
    "ax = subplot(1,1,1)\n",
    "ax.plot(range(0,set_num), result_mc, linewidth=2, color='g')\n",
    "\n",
    "plt.ylabel(\"Set average reward\");\n",
    "plt.xlabel(\"Set number (2000 episodes simulated in each set)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use greedy policy of the trained Q function to control the carpole for 100 episode, \n",
    "# and plot the total reward received in each episode\n",
    "## Suggested flow (Feel free to modify and add)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "\n",
    "count = 0\n",
    "while 1:\n",
    "    \n",
    "    current_state =                             # discretize the observation space\n",
    "    \n",
    "    action =                                    # choose action by greedy policy of the trained Q\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        observation = env.reset()\n",
    "        count +=1\n",
    "        \n",
    "        total_reward_mc =                                # record the total reward until this episode\n",
    "        \n",
    "        if count==100:\n",
    "            break\n",
    "        \n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "figure(figsize=(12,6))\n",
    "ax = subplot(1,1,1)\n",
    "ax.plot(range(100), total_reward_mc)\n",
    "\n",
    "plt.ylabel(\"Accumulate Reward\");\n",
    "plt.xlabel(\"Episode\");\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
