{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# EN.520.637 Foundations of Reinforcement Learning\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Lab 5: SARSA and Q-learning  </font></h1>\n",
    "\n",
    "\n",
    "\n",
    "## Deadline\n",
    "11:59 pm Nov 12th, 2021 \n",
    "\n",
    "##  Content\n",
    "1. Cliff walking example\n",
    "2. Car pole example\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Gym and other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cliff walk example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Intro to Cliff walk \n",
    "\n",
    "In this section, we use SARSA and Q-learning algorithm to solve to a cliff walk problem. ( See Sutton&Barto Example 6.6 )\n",
    "\n",
    "The grid is shown below, the black tiles represents wall/obstacles, the white tiles are the non-terminal tiles, and the tile with \"s\" is the starting point of every episoid, the tile with \"G\" is the goal point.\n",
    "\n",
    "The agent start at \"s\" tile. At every step, the agent can choose one of the four actions:\"up\",\"right\",\"down\",\"left\", moving to the next tile in that direction. \n",
    "\n",
    "$\\cdot$ If the next tile is wall/obstacle, the agent does not move and receive -1 reward;\n",
    "\n",
    "$\\cdot$ If the next tile is a non-terminal tile, the agent move to that tile and receive 0 reward;\n",
    "\n",
    "$\\cdot$ If the next tile is the goal tile, the episoid is finished and the agent receive 100 reward (set to be 100 to accelarate the training).\n",
    "\n",
    "$\\cdot$ If the next tile is the cliff, the episoid is finished and the agent receive -100 reward ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld2 import GridWorld\n",
    "\n",
    "gw = GridWorld()\n",
    "gw.plot_grid(plot_title='The grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 States and state values\n",
    "Excluding the wall around the grid, there are 32 tiles (INCLUDING obstacles inside the grid), and they correspond to 32 states (obstacles and goal are non-reachable states).\n",
    "\n",
    "We use numbers from 0 to 24 to represent these states (see gridworld.py for the coversion between integer and tile position). The correspondance are as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gw.plot_state_values(np.arange(25),value_format=\"{:d}\",plot_title='Each states as an integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Take actions\n",
    "Use GridWorld.step(action) to take an action, and use GridWorld.reset() to restart an episoid\n",
    "\n",
    "action is an integer from 0 to 3\n",
    "\n",
    "0: \"Up\"; 1: \"Right\"; 2: \"Down\"; 3: \"Left\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current state is 24, which corresponds to tile position (3, 0)\n",
      "\n",
      "Take action 3, get reward -1, move to state 24\n",
      "Now the current state is 24, which corresponds to tile position (3, 0)\n",
      "\n",
      "Reset episode\n",
      "Now the current state is 24, which corresponds to tile position (3, 0)\n"
     ]
    }
   ],
   "source": [
    "gw.reset()\n",
    "\n",
    "current_state = gw.get_current_state()\n",
    "tile_pos = gw.int_to_state(current_state)\n",
    "\n",
    "print(\"The current state is {}, which corresponds to tile position {}\\n\".format(current_state,tile_pos))\n",
    "\n",
    "action = np.random.randint(4)\n",
    "reward, terminated, next_state = gw.step(action)\n",
    "tile_pos = gw.int_to_state(next_state)\n",
    "\n",
    "print(\"Take action {}, get reward {}, move to state {}\".format(action,reward,next_state))\n",
    "print(\"Now the current state is {}, which corresponds to tile position {}\\n\".format(next_state,tile_pos))\n",
    "\n",
    "gw.reset()\n",
    "current_state = gw.get_current_state()\n",
    "tile_pos = gw.int_to_state(current_state)\n",
    "print(\"Reset episode\")\n",
    "print(\"Now the current state is {}, which corresponds to tile position {}\".format(current_state,tile_pos))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Plot Deterministic Policies\n",
    "A deterministic policy is a function from state to action, which can be represented by a (32,)-numpy array whose entries are all integers in (0-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gw.plot_policy(np.random.randint(4,size=(32,)),plot_title='A deterministic policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SARSA & Q_learning\n",
    "\n",
    "1. Implement SARSA algorithm (See Sutton&Barto Section 6.4) on this example for 5000 episodes to learn the optimal policy. \n",
    "   Plot the greedy policy of the learned Q-function using gw.plot_policy()\n",
    "\n",
    "0. Implement Q_learning algorithm (See Sutton&Barto Section 6.5) on this example for 5000 episodes to learn the optimal policy. \n",
    "   Plot the greedy policy of the learned Q-function using gw.plot_policy()\n",
    "0. Plot the total rewards during one episode v.s. number of episodes trained for both SARSA and Q-Learning. Compare the plot to [Sutton & Barto Figure 6.4] (Optional)You may  [1]. Smooth your curve by taking the average of total rewards over successive 50 episodes [2].Avoid adding the artificial \"+100\" goal reward to the total reward to match you figure with the book (Although we need to used goal reward when update the Q-function )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested functions (Feel free to modify existing and add new functions)\n",
    "\n",
    "\n",
    "def update_Q(Q, current_idx, next_idx, current_action, next_action, alpha, R, gamma):\n",
    "    # Update Q at the each step\n",
    "    #\n",
    "    # input:  current Q,                    (array) \n",
    "    #         current_idx, next_idx         (array)  states  \n",
    "    #         current_action, next_action   (array)  actions  \n",
    "    #         alpha, R, gamma               (floats) learning rate, reward, discount rate\n",
    "    # output: Updated Q\n",
    "    #         \n",
    "    return Q\n",
    "\n",
    "def get_action(current_idx, Q, epsilon):\n",
    "    \n",
    "    # Choose optimal action based on current state and Q\n",
    "    #\n",
    "    # input:  current_idx     (array) \n",
    "    #         Q,              (array)  \n",
    "    #         epsilon,        (float)  \n",
    "    # output: action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested flow (Feel free to modify and add)\n",
    "## SARSA\n",
    "Q  = np.zeros((25,4))\n",
    "\n",
    "gw.reset()\n",
    "\n",
    "max_ep = 5000\n",
    "\n",
    "total_reward_sarsa = np.zeros(max_ep)\n",
    "\n",
    "epsilon = 0.1\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "for ep in range(0, max_ep):\n",
    "    gw.reset()\n",
    "    terminated = False\n",
    "\n",
    "    \n",
    "\n",
    "    while terminated == False:\n",
    "        reward, terminated, next_state = gw.step(current_action)\n",
    "        if not reward == 100: total_reward_sarsa[ep] += reward\n",
    "        next_action = get_action(next_state,Q,epsilon)\n",
    "        \n",
    "        Q = update_Q(Q, current_state, next_state, current_action, next_action, alpha, reward, gamma)\n",
    "\n",
    "        \n",
    "        current_state = next_state\n",
    "        current_action = next_action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested flow (Feel free to modify and add)\n",
    "## Q_learning\n",
    "Q  = np.zeros((25,4))\n",
    "\n",
    "\n",
    "gw.reset()\n",
    "\n",
    "\n",
    "max_ep = 5000\n",
    "\n",
    "total_reward_qlearning = np.zeros(max_ep)\n",
    "\n",
    "epsilon = 0.1\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "for ep in range(0, max_ep):\n",
    "    gw.reset()\n",
    "    terminated = False\n",
    "\n",
    "    \n",
    "\n",
    "    while terminated == False:\n",
    "        reward, terminated, next_state = gw.step(current_action)\n",
    "        if not reward == 100: total_reward_qlearning[ep] += reward\n",
    "        max_action = get_action(next_state,Q,0)\n",
    "\n",
    "        \n",
    "        Q = update_Q(Q, current_state, next_state, current_action, max_action, alpha, reward, gamma)\n",
    "\n",
    "        \n",
    "        current_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CartPole-v1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 CartPole Introduction\n",
    "\n",
    "We now use SARSA and Q-learning to the CartPole problem. \n",
    "\n",
    "\n",
    "1. A pole is attached via an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "\n",
    "0. The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "\n",
    "0. The pole starts at upright position, and the goal is to prevent it from falling over. \n",
    "\n",
    "0. A reward of +1 is obtained for every timestep that the pole remains upright. \n",
    "\n",
    "0. The episode ends when the pole is more than 15 degrees from the vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "0. For more info (See [SOURCE ON GITHUB](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)).\n",
    "\n",
    "The following examples show the basic usage of this testing environment: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Episode initialization and Initial Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inital observation is [-0.04053095  0.00310971  0.04278405  0.01623798]\n",
      "\n",
      "This means the cart current position is -0.0405309534847064 with velocity 0.0031097121084252347,\n",
      "and the pole current angular position is 0.04278404555046254 with angular velocity 0.016237983913741066,\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset() ##Initial an episode\n",
    "\n",
    "print(\"Inital observation is {}\".format(observation))\n",
    "\n",
    "print(\"\\nThis means the cart current position is {}\".format(observation[0]), end = '')\n",
    "print(\" with velocity {},\".format(observation[1]))\n",
    "\n",
    "print(\"and the pole current angular position is {}\".format(observation[2]), end = '')\n",
    "print(\" with angular velocity {},\".format(observation[3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Take actions\n",
    "\n",
    "\n",
    "Use env.step(action) to take an action\n",
    "\n",
    "action is an integer from 0 to 1\n",
    "\n",
    "0: \"Left\"; 1: \"Right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current observation is [-0.04053095  0.00310971  0.04278405  0.01623798]\n",
      "\n",
      "New observation is [-0.04046876 -0.19259887  0.04310881  0.32210689]\n",
      "Step reward is 1.0\n",
      "Did episode just ends? -False\n"
     ]
    }
   ],
   "source": [
    "print(\"Current observation is {}\".format(observation))\n",
    "\n",
    "action = 0 #go left\n",
    "observation, reward, done, info = env.step(action) # simulate one step\n",
    "\n",
    "print(\"\\nNew observation is {}\".format(observation))\n",
    "print(\"Step reward is {}\".format(reward))\n",
    "print(\"Did episode just ends? -{}\".format(done)) # episode ends when 3.1(6) happens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Simulate multiple episodes\n",
    "\n",
    "(You may uncomment those lines to see an animation. However, it will not work for JupyterHub since the animation requires GL instead of webGL. If you have Jupyter notebook localy on your computer, this version of code will work through a virtual frame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode is 22.22222222222222\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "total_reward = 0\n",
    "ep_num = 0\n",
    "# img = plt.imshow(env.render(mode='rgb_array')) \n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    #     img.set_data(env.render(mode='rgb_array')) \n",
    "    #     display.display(plt.gcf())\n",
    "    #     display.clear_output(wait=True)\n",
    "    \n",
    "    action = env.action_space.sample()     # this takes random actions\n",
    "    observation, reward, done, info = env.step(action) \n",
    "       \n",
    "    total_reward += reward\n",
    "    \n",
    "\n",
    "\n",
    "    if done:                               # episode just ends\n",
    "        observation = env.reset()          # reset episode\n",
    "        ep_num += 1\n",
    "\n",
    "print(\"Average reward per episode is {}\".format(total_reward/ep_num))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 States Discretization \n",
    "\n",
    "The class DiscreteObs() discretizes the observation space into discrete state space, based on numpy.digitize (Please read its description in https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) \n",
    "\n",
    "Discretization of observation space is necessary for tabular methods. You can use DiscreteObs() or any other library for discretizing the observation space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position of the cart is -0.0133\n",
      "\n",
      "Current velocity of the cart is -0.0271\n",
      "\n",
      "Current angular position of the pole is 0.0241 rad\n",
      "\n",
      "Current angular velocity of the pole is -0.0017 rad\n",
      "\n",
      "which are mapped to state [0, 0, 3, 5], with corresponding index 33\n",
      "index 33 maps to state[0, 0, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "class DiscretObs():\n",
    "    \n",
    "    \n",
    "    def __init__(self, bins_list):\n",
    "        self._bins_list = bins_list\n",
    "        \n",
    "        self._bins_num = len(bins_list)\n",
    "        self._state_num_list = [len(bins)+1 for bins in bins_list]\n",
    "        self._state_num_total = np.prod(self._state_num_list)\n",
    "    \n",
    "    def get_state_num_total(self):\n",
    "        \n",
    "        return self._state_num_total\n",
    "    \n",
    "    def _state_num_list(self):\n",
    "        \n",
    "        return self._state_num_list\n",
    "    \n",
    "    def obs2state(self, obs):\n",
    "        \n",
    "        if not len(obs)==self._bins_num:\n",
    "            raise ValueError(\"observation must have length {}\".format(self._bins_num))\n",
    "        else:\n",
    "            return [np.digitize(obs[i], bins=self._bins_list[i]) for i in range(self._bins_num)]\n",
    "        \n",
    "    def obs2idx(self, obs):\n",
    "        \n",
    "        state = self.obs2state(obs)\n",
    "        \n",
    "        return self.state2idx(state)\n",
    "    \n",
    "    def state2idx(self, state):\n",
    "        \n",
    "        idx = 0\n",
    "        for i in range(self._bins_num-1,-1,-1):\n",
    "            idx = idx*self._state_num_list[i]+state[i]\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "    def idx2state(self, idx):\n",
    "        \n",
    "        state = [None]*self._bins_num\n",
    "        state_num_cumul = np.cumprod(self._state_num_list)\n",
    "        for i in range(self._bins_num-1,0,-1):\n",
    "            state[i] = idx//state_num_cumul[i-1]\n",
    "            idx -=state[i]*state_num_cumul[i-1]\n",
    "        state[0] = idx%state_num_cumul[0]\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Recommended epsilon and learning_rate update (Feel free to modify existing and add new functions)\n",
    "def get_epsilon(t):\n",
    "    return max(0.1, min(1., 1. - math.log10((t + 1) / 25)))\n",
    "\n",
    "def get_learning_rate(t):\n",
    "    return max(0.1, min(1., 1. - math.log10((t + 1) / 25)))\n",
    "\n",
    "\n",
    "\n",
    "# Recommended Discretization for Carpole-v1 when using Monte-Carlo methods\n",
    "bins_pos = []                                       # position\n",
    "bins_d_pos = []                                     # velocity\n",
    "bins_ang = np.linspace(-0.41887903,0.41887903,5)    # angle\n",
    "bins_d_ang = np.linspace(-0.87266,0.87266,11)       # angular velocity\n",
    "\n",
    "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
    "observation = env.reset()\n",
    "\n",
    "state = dobs.obs2state(observation)\n",
    "\n",
    "idx = dobs.state2idx(state)\n",
    "\n",
    "\n",
    "print(\"Current position of the cart is {:.4f}\\n\".format(observation[0]))\n",
    "print(\"Current velocity of the cart is {:.4f}\\n\".format(observation[1]))\n",
    "print(\"Current angular position of the pole is {:.4f} rad\\n\".format(observation[2]))\n",
    "print(\"Current angular velocity of the pole is {:.4f} rad\\n\".format(observation[3]))\n",
    "\n",
    "print(\"which are mapped to state {}, with corresponding index {}\".format(state,idx))\n",
    "print(\"index {} maps to state{}\".format(idx,dobs.idx2state(idx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 SARSA & Q_learning\n",
    "\n",
    "1. Implement SARSA algorithm (See Sutton&Barto Section 6.4) on this example for 1000 episodes to learn the optimal policy. \n",
    "\n",
    "0. Divide the 1000 traing episodes into 50 sets. Plot the average reward for each set. (i.e. plot the average reward for the first 20 episodes, the second 20 episodes, ..., and the 50th 20 episodes.) \n",
    "\n",
    "0. Implement Q_learning algorithm (See Sutton&Barto Section 6.5) on this example for 1000 episodes to to learn the optimal policy.\n",
    "\n",
    "0. Divide the 1000 traing episodes into 50 sets. Plot the average reward for each set. (i.e. plot the average reward for the first 20 episodes, the second 20 episodes, ..., and the 50th 20 episodes.) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested functions (Feel free to modify existing and add new functions)\n",
    "\n",
    "\n",
    "def update_Q(Q, current_idx, next_idx, current_action, next_action, alpha, R, gamma):\n",
    "    # Update Q at the each step\n",
    "    #\n",
    "    # input:  current Q,                    (array) \n",
    "    #         current_idx, next_idx         (array)  states  \n",
    "    #         current_action, next_action   (array)  actions  \n",
    "    #         alpha, R, gamma               (floats) learning rate, reward, discount rate\n",
    "    # output: Updated Q\n",
    "    #         \n",
    "    return Q\n",
    "\n",
    "def get_action(current_idx, Q, epsilon):\n",
    "    \n",
    "    # Choose optimal action based on current state and Q\n",
    "    #\n",
    "    # input:  current_idx     (array) \n",
    "    #         Q,              (array)  \n",
    "    #         epsilon,        (float)  \n",
    "    # output: action\n",
    "    return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested flow (Feel free to modify and add)\n",
    "## SARSA\n",
    "total_reward = 0\n",
    "\n",
    "bins_pos = []\n",
    "bins_d_pos = []\n",
    "bins_ang = np.linspace(-0.41887903,0.41887903,5)\n",
    "bins_d_ang = np.linspace(-0.87266,0.87266,11)\n",
    "\n",
    "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "\n",
    "# Q defined by states\n",
    "# Q = np.zeros((2,dobs._state_num_list[0],dobs._state_num_list[1],dobs._state_num_list[2],dobs._state_num_list[3]))\n",
    "# Q defined by index\n",
    "# Q = np.zeros((2,dobs.get_state_num_total())\n",
    "             \n",
    "count = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gamma = 0.98\n",
    "result = np.zeros(50)\n",
    "s = 0\n",
    "for ep in range(1000):\n",
    "    if  np.mod(ep,20)==0:\n",
    "        result[s] = total_reward/20\n",
    "        s+=1\n",
    "        total_reward = 0\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "#     current_state = dobs.obs2state(observation)\n",
    "#     current_idx = dobs.obs2idx(observation)\n",
    "    \n",
    "    alpha = get_learning_rate(ep)\n",
    "    epsilon = get_epsilon(ep)\n",
    "\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        total_reward += 1\n",
    "        action = \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "#         next_idx = \n",
    "#         next_state = \n",
    "        next_action = \n",
    "        \n",
    "        Q = update_Q(Q, current_idx, next_idx, action, next_action, alpha, reward, gamma)\n",
    "        current_idx = next_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested flow (Feel free to modify and add)\n",
    "## Q_learning\n",
    "total_reward = 0\n",
    "\n",
    "bins_pos = []\n",
    "bins_d_pos = []\n",
    "bins_ang = np.linspace(-0.41887903,0.41887903,5)\n",
    "bins_d_ang = np.linspace(-0.87266,0.87266,11)\n",
    "\n",
    "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "\n",
    "\n",
    "# Q defined by states\n",
    "# Q = np.zeros((2,dobs._state_num_list[0],dobs._state_num_list[1],dobs._state_num_list[2],dobs._state_num_list[3]))\n",
    "# Q defined by index\n",
    "# Q = np.zeros((2,dobs.get_state_num_total())\n",
    "\n",
    "gamma = 0.98\n",
    "result = np.zeros(50)\n",
    "s = 0\n",
    "for ep in range(1000):\n",
    "    if  np.mod(ep,20)==0:\n",
    "        result[s] = total_reward/20\n",
    "        s+=1\n",
    "        total_reward = 0\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "#     current_state = dobs.obs2state(observation)\n",
    "#     current_idx = dobs.obs2idx(observation)\n",
    "\n",
    "    alpha = get_learning_rate(ep)\n",
    "    epsilon = get_epsilon(ep)\n",
    "\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        total_reward += 1\n",
    "        action = \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "#         next_idx = \n",
    "#         nex_state = \n",
    "        max_action = \n",
    "        \n",
    "        Q = update_Q(Q, current_idx, next_idx, action, max_action, alpha, reward, gamma)\n",
    "        current_idx = next_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
