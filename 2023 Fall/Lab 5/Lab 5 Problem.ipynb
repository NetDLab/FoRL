{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# EN.520.637 Foundations of Reinforcement Learning\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Lab 5: SARSA and Q-learning  </font></h1>\n",
    "\n",
    "\n",
    "\n",
    "## Deadline\n",
    "11:59 pm Nov 14th, 2022 \n",
    "\n",
    "##  Content\n",
    "1. Cliff walking example (50 points)\n",
    "2. Cart pole example (50 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Gym and other necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cliff-walk example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Intro to Cliff walk \n",
    "\n",
    "In this section, we use SARSA and Q-learning algorithm to solve to a cliff walk problem. ( See Sutton&Barto Example 6.6 )\n",
    "\n",
    "The grid is shown below, the black tiles represents wall/obstacles, the white tiles are the non-terminal tiles, and the tile with \"s\" is the starting point of every episoid, the tile with \"G\" is the goal point.\n",
    "\n",
    "The agent start at \"s\" tile. At every step, the agent can choose one of the four actions:\"up\",\"right\",\"down\",\"left\", moving to the next tile in that direction. \n",
    "\n",
    "$\\cdot$ If the next tile is wall/obstacle, the agent does not move and receive -1 reward;\n",
    "\n",
    "$\\cdot$ If the next tile is a non-terminal tile, the agent move to that tile and receive 0 reward;\n",
    "\n",
    "$\\cdot$ If the next tile is the goal tile, the episoid is finished and the agent receive 100 reward (set to be 100 to accelarate the training).\n",
    "\n",
    "$\\cdot$ If the next tile is the cliff, the episoid is finished and the agent receive -100 reward ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agu/opt/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld2 import GridWorld\n",
    "\n",
    "gw = GridWorld()\n",
    "gw.plot_grid(plot_title='The grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### States and state values\n",
    "Excluding the wall around the grid, there are 32 tiles (INCLUDING obstacles inside the grid), and they correspond to 32 states (obstacles and goal are non-reachable states).\n",
    "\n",
    "We use numbers from 0 to 24 to represent these states (see gridworld.py for the coversion between integer and tile position). The correspondance are as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gw.plot_state_values(np.arange(25),value_format=\"{:d}\",plot_title='Each state as an integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions\n",
    "Use GridWorld.step(action) to take an action, and use GridWorld.reset() to restart an episoid\n",
    "\n",
    "action is an integer from 0 to 3\n",
    "\n",
    "0: \"Up\"; 1: \"Right\"; 2: \"Down\"; 3: \"Left\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current state is 24, which corresponds to tile position (3, 0)\n",
      "\n",
      "Take action 0, get reward -1.0, move to state 16\n",
      "Now the current state is 16, which corresponds to tile position (2, 0)\n",
      "\n",
      "Reset episode\n",
      "Now the current state is 24, which corresponds to tile position (3, 0)\n"
     ]
    }
   ],
   "source": [
    "gw.reset()\n",
    "\n",
    "current_state = gw.get_current_state()\n",
    "tile_pos = gw.int_to_state(current_state)\n",
    "\n",
    "print(\"The current state is {}, which corresponds to tile position {}\\n\".format(current_state,tile_pos))\n",
    "\n",
    "action = np.random.randint(4)\n",
    "reward, terminated, next_state = gw.step(action)\n",
    "tile_pos = gw.int_to_state(next_state)\n",
    "\n",
    "print(\"Take action {}, get reward {}, move to state {}\".format(action,reward,next_state))\n",
    "print(\"Now the current state is {}, which corresponds to tile position {}\\n\".format(next_state,tile_pos))\n",
    "\n",
    "gw.reset()\n",
    "current_state = gw.get_current_state()\n",
    "tile_pos = gw.int_to_state(current_state)\n",
    "print(\"Reset episode\")\n",
    "print(\"Now the current state is {}, which corresponds to tile position {}\".format(current_state,tile_pos))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Deterministic Policies\n",
    "A deterministic policy is a function from state to action, which can be represented by a (32,)-numpy array whose entries are all integers in (0-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gw.plot_policy(np.random.randint(4,size=(32,)),plot_title='A deterministic policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 SARSA & Q_learning \n",
    "\n",
    "You will now implement Sarsa and Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested functions (Feel free to modify existing and add new functions)\n",
    "\n",
    "\n",
    "def update_Q(Q, current_idx, next_idx, current_action, next_action, alpha, R, gamma):\n",
    "    # Update Q at the each step\n",
    "    #\n",
    "    # input:  current Q,                    (array) \n",
    "    #         current_idx, next_idx         (array)  states  \n",
    "    #         current_action, next_action   (array)  actions  \n",
    "    #         alpha, R, gamma               (floats) learning rate, reward, discount rate\n",
    "    # output: Updated Q\n",
    "    #         \n",
    "    return Q\n",
    "\n",
    "def get_action(current_idx, Q, epsilon):\n",
    "    \n",
    "    # Choose optimal action based on current state and Q\n",
    "    #\n",
    "    # input:  current_idx     (array) \n",
    "    #         Q,              (array)  \n",
    "    #         epsilon,        (float)  \n",
    "    # output: action\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task] 1.2.1 SARSA [Coding: 20 points]\n",
    "\n",
    "* Implement SARSA (See Sutton&Barto Section 6.4) on this example for 5000 episodes to learn the optimal policy. \n",
    "* Plot the greedy policy of the learned Q-function using gw.plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'current_action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bb640824f692>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtotal_reward_sarsa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnext_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'current_action' is not defined"
     ]
    }
   ],
   "source": [
    "## Suggested flow (Feel free to modify and add)\n",
    "## SARSA\n",
    "Q  = np.zeros((25,4))\n",
    "\n",
    "gw.reset()\n",
    "\n",
    "max_ep = 5000\n",
    "\n",
    "total_reward_sarsa = np.zeros(max_ep)\n",
    "\n",
    "epsilon = 0.1\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "for ep in range(0, max_ep):\n",
    "    gw.reset()\n",
    "    terminated = False\n",
    "\n",
    "    \n",
    "\n",
    "    while terminated == False:\n",
    "        reward, terminated, next_state = gw.step(current_action)\n",
    "        if not reward == 100: total_reward_sarsa[ep] += reward\n",
    "        next_action = get_action(next_state,Q,epsilon)\n",
    "        \n",
    "        Q = update_Q(Q, current_state, next_state, current_action, next_action, alpha, reward, gamma)\n",
    "\n",
    "        \n",
    "        current_state = next_state\n",
    "        current_action = next_action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task] 1.2.2 Q-learning [Coding: 20 points]\n",
    "* Implement Q_learning algorithm (See Sutton&Barto Section 6.5) on this example for 5000 episodes to learn the optimal policy. \n",
    "* Plot the greedy policy of the learned Q-function using gw.plot_policy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested flow (Feel free to modify and add)\n",
    "## Q_learning\n",
    "Q  = np.zeros((25,4))\n",
    "\n",
    "\n",
    "gw.reset()\n",
    "\n",
    "\n",
    "max_ep = 5000\n",
    "\n",
    "total_reward_qlearning = np.zeros(max_ep)\n",
    "\n",
    "epsilon = 0.1\n",
    "alpha = 0.5\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "for ep in range(0, max_ep):\n",
    "    gw.reset()\n",
    "    terminated = False\n",
    "\n",
    "    \n",
    "\n",
    "    while terminated == False:\n",
    "        reward, terminated, next_state = gw.step(current_action)\n",
    "        if not reward == 100: total_reward_qlearning[ep] += reward\n",
    "        max_action = get_action(next_state,Q,0)\n",
    "\n",
    "        \n",
    "        Q = update_Q(Q, current_state, next_state, current_action, max_action, alpha, reward, gamma)\n",
    "\n",
    "        \n",
    "        current_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 1.2.3] Comparison [Coding/Question 10 points]\n",
    "* Plot the total rewards during one episode v.s. number of episodes trained for both SARSA and Q-Learning. \n",
    "\n",
    "* Compare your plot to the one in [Sutton & Barto Figure 6.4].\n",
    "\n",
    "* Which algorithm obtains bette performance? Provide some intuition on why this is the case.\n",
    "\n",
    "(Optional)You may  \n",
    "\n",
    "1. Smooth your curve by taking the average of total rewards over successive 50 episodes \n",
    "2. Avoid adding the artificial \"+100\" goal reward to the total reward to match you figure with the book (Although we need to used goal reward when update the Q-function )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Your written answer/comparison here***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CartPole-v1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 CartPole Introduction\n",
    "\n",
    "We now use SARSA and Q-learning on the CartPole problem. \n",
    "\n",
    "\n",
    "1. A pole is attached via an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "\n",
    "0. The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "\n",
    "0. The pole starts at upright position, and the goal is to prevent it from falling over. \n",
    "\n",
    "0. A reward of +1 is obtained for every timestep that the pole remains upright. \n",
    "\n",
    "0. The episode ends when the pole is more than 15 degrees from the vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "0. For more info (See [SOURCE ON GITHUB](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)).\n",
    "\n",
    "The following examples show the basic usage of this testing environment: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode initialization and Initial Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset() ##Initial an episode\n",
    "\n",
    "print(\"Inital observation is {}\".format(observation))\n",
    "\n",
    "print(\"\\nThis means the cart current position is {}\".format(observation[0]), end = '')\n",
    "print(\" with velocity {},\".format(observation[1]))\n",
    "\n",
    "print(\"and the pole current angular position is {}\".format(observation[2]), end = '')\n",
    "print(\" with angular velocity {},\".format(observation[3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking actions\n",
    "\n",
    "\n",
    "Use env.step(action) to take an action\n",
    "\n",
    "action is an integer from 0 to 1\n",
    "\n",
    "0: \"Left\"; 1: \"Right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current observation is {}\".format(observation))\n",
    "\n",
    "action = 0 #go left\n",
    "observation, reward, done, info = env.step(action) # simulate one step\n",
    "\n",
    "print(\"\\nNew observation is {}\".format(observation))\n",
    "print(\"Step reward is {}\".format(reward))\n",
    "print(\"Did episode just ends? -{}\".format(done)) # episode ends when 3.1(6) happens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating multiple episodes\n",
    "\n",
    "(You may uncomment those lines to see an animation. However, it will not work for JupyterHub since the animation requires GL instead of webGL. If you have Jupyter notebook localy on your computer, this version of code will work through a virtual frame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "total_reward = 0\n",
    "ep_num = 0\n",
    "# img = plt.imshow(env.render(mode='rgb_array')) \n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    #     img.set_data(env.render(mode='rgb_array')) \n",
    "    #     display.display(plt.gcf())\n",
    "    #     display.clear_output(wait=True)\n",
    "    \n",
    "    action = env.action_space.sample()     # this takes random actions\n",
    "    observation, reward, done, info = env.step(action) \n",
    "       \n",
    "    total_reward += reward\n",
    "    \n",
    "\n",
    "\n",
    "    if done:                               # episode just ends\n",
    "        observation = env.reset()          # reset episode\n",
    "        ep_num += 1\n",
    "\n",
    "print(\"Average reward per episode is {}\".format(total_reward/ep_num))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 States Discretization \n",
    "\n",
    "The class DiscreteObs() discretizes the observation space into discrete state space, based on numpy.digitize (Please read its description in https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) \n",
    "\n",
    "Discretization of observation space is necessary for tabular methods. You can use DiscreteObs() or any other library for discretizing the observation space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretObs():\n",
    "    \n",
    "    \n",
    "    def __init__(self, bins_list):\n",
    "        self._bins_list = bins_list\n",
    "        \n",
    "        self._bins_num = len(bins_list)\n",
    "        self._state_num_list = [len(bins)+1 for bins in bins_list]\n",
    "        self._state_num_total = np.prod(self._state_num_list)\n",
    "    \n",
    "    def get_state_num_total(self):\n",
    "        \n",
    "        return self._state_num_total\n",
    "    \n",
    "    def _state_num_list(self):\n",
    "        \n",
    "        return self._state_num_list\n",
    "    \n",
    "    def obs2state(self, obs):\n",
    "        \n",
    "        if not len(obs)==self._bins_num:\n",
    "            raise ValueError(\"observation must have length {}\".format(self._bins_num))\n",
    "        else:\n",
    "            return [np.digitize(obs[i], bins=self._bins_list[i]) for i in range(self._bins_num)]\n",
    "        \n",
    "    def obs2idx(self, obs):\n",
    "        \n",
    "        state = self.obs2state(obs)\n",
    "        \n",
    "        return self.state2idx(state)\n",
    "    \n",
    "    def state2idx(self, state):\n",
    "        \n",
    "        idx = 0\n",
    "        for i in range(self._bins_num-1,-1,-1):\n",
    "            idx = idx*self._state_num_list[i]+state[i]\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "    def idx2state(self, idx):\n",
    "        \n",
    "        state = [None]*self._bins_num\n",
    "        state_num_cumul = np.cumprod(self._state_num_list)\n",
    "        for i in range(self._bins_num-1,0,-1):\n",
    "            state[i] = idx//state_num_cumul[i-1]\n",
    "            idx -=state[i]*state_num_cumul[i-1]\n",
    "        state[0] = idx%state_num_cumul[0]\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Recommended epsilon and learning_rate update (Feel free to modify existing and add new functions)\n",
    "def get_epsilon(t):\n",
    "    return max(0.1, min(1., 1. - math.log10((t + 1) / 25)))\n",
    "\n",
    "def get_learning_rate(t):\n",
    "    return max(0.1, min(1., 1. - math.log10((t + 1) / 25)))\n",
    "\n",
    "\n",
    "\n",
    "# Recommended Discretization for Carpole-v1 when using Monte-Carlo methods\n",
    "bins_pos = []                                       # position\n",
    "bins_d_pos = []                                     # velocity\n",
    "bins_ang = np.linspace(-0.41887903,0.41887903,5)    # angle\n",
    "bins_d_ang = np.linspace(-0.87266,0.87266,11)       # angular velocity\n",
    "\n",
    "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
    "observation = env.reset()\n",
    "\n",
    "state = dobs.obs2state(observation)\n",
    "\n",
    "idx = dobs.state2idx(state)\n",
    "\n",
    "\n",
    "print(\"Current position of the cart is {:.4f}\\n\".format(observation[0]))\n",
    "print(\"Current velocity of the cart is {:.4f}\\n\".format(observation[1]))\n",
    "print(\"Current angular position of the pole is {:.4f} rad\\n\".format(observation[2]))\n",
    "print(\"Current angular velocity of the pole is {:.4f} rad\\n\".format(observation[3]))\n",
    "\n",
    "print(\"which are mapped to state {}, with corresponding index {}\".format(state,idx))\n",
    "print(\"index {} maps to state{}\".format(idx,dobs.idx2state(idx)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 SARSA & Q_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested functions (Feel free to modify existing and add new functions)\n",
    "\n",
    "\n",
    "def update_Q(Q, current_idx, next_idx, current_action, next_action, alpha, R, gamma):\n",
    "    # Update Q at the each step\n",
    "    #\n",
    "    # input:  current Q,                    (array) \n",
    "    #         current_idx, next_idx         (array)  states  \n",
    "    #         current_action, next_action   (array)  actions  \n",
    "    #         alpha, R, gamma               (floats) learning rate, reward, discount rate\n",
    "    # output: Updated Q\n",
    "    #         \n",
    "    return Q\n",
    "\n",
    "def get_action(current_idx, Q, epsilon):\n",
    "    \n",
    "    # Choose optimal action based on current state and Q\n",
    "    #\n",
    "    # input:  current_idx     (array) \n",
    "    #         Q,              (array)  \n",
    "    #         epsilon,        (float)  \n",
    "    # output: action\n",
    "    return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2.2.1] SARSA [Coding, 20 points]\n",
    "\n",
    "Implement SARSA algorithm (See Sutton&Barto Section 6.4) on this example for 1000 episodes to learn the optimal policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested flow (Feel free to modify and add)\n",
    "## SARSA\n",
    "total_reward = 0\n",
    "\n",
    "bins_pos = []\n",
    "bins_d_pos = []\n",
    "bins_ang = np.linspace(-0.41887903,0.41887903,5)\n",
    "bins_d_ang = np.linspace(-0.87266,0.87266,11)\n",
    "\n",
    "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "\n",
    "# Q defined by states\n",
    "# Q = np.zeros((2,dobs._state_num_list[0],dobs._state_num_list[1],dobs._state_num_list[2],dobs._state_num_list[3]))\n",
    "# Q defined by index\n",
    "# Q = np.zeros((2,dobs.get_state_num_total())\n",
    "             \n",
    "count = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gamma = 0.98\n",
    "result = np.zeros(50)\n",
    "s = 0\n",
    "for ep in range(1000):\n",
    "    if  np.mod(ep,20)==0:\n",
    "        result[s] = total_reward/20\n",
    "        s+=1\n",
    "        total_reward = 0\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "#     current_state = dobs.obs2state(observation)\n",
    "#     current_idx = dobs.obs2idx(observation)\n",
    "    \n",
    "    alpha = get_learning_rate(ep)\n",
    "    epsilon = get_epsilon(ep)\n",
    "\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        total_reward += 1\n",
    "        action = \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "#         next_idx = \n",
    "#         next_state = \n",
    "        next_action = \n",
    "        \n",
    "        Q = update_Q(Q, current_idx, next_idx, action, next_action, alpha, reward, gamma)\n",
    "        current_idx = next_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2.2.2] Coding [10 points]\n",
    "Divide the 1000 traing episodes into 50 sets. Plot the average reward for each set. (i.e. plot the average reward for the first 20 episodes, the second 20 episodes, ..., and the 50th 20 episodes.) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2.2.3] Q-learning [Coding, 20 points]\n",
    "\n",
    "Implement Q_learning algorithm (See Sutton&Barto Section 6.5) on this example for 1000 episodes to to learn the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested flow (Feel free to modify and add)\n",
    "## Q_learning\n",
    "total_reward = 0\n",
    "\n",
    "bins_pos = []\n",
    "bins_d_pos = []\n",
    "bins_ang = np.linspace(-0.41887903,0.41887903,5)\n",
    "bins_d_ang = np.linspace(-0.87266,0.87266,11)\n",
    "\n",
    "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "\n",
    "\n",
    "# Q defined by states\n",
    "# Q = np.zeros((2,dobs._state_num_list[0],dobs._state_num_list[1],dobs._state_num_list[2],dobs._state_num_list[3]))\n",
    "# Q defined by index\n",
    "# Q = np.zeros((2,dobs.get_state_num_total())\n",
    "\n",
    "gamma = 0.98\n",
    "result = np.zeros(50)\n",
    "s = 0\n",
    "for ep in range(1000):\n",
    "    if  np.mod(ep,20)==0:\n",
    "        result[s] = total_reward/20\n",
    "        s+=1\n",
    "        total_reward = 0\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "#     current_state = dobs.obs2state(observation)\n",
    "#     current_idx = dobs.obs2idx(observation)\n",
    "\n",
    "    alpha = get_learning_rate(ep)\n",
    "    epsilon = get_epsilon(ep)\n",
    "\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        total_reward += 1\n",
    "        action = \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        \n",
    "#         next_idx = \n",
    "#         nex_state = \n",
    "        max_action = \n",
    "        \n",
    "        Q = update_Q(Q, current_idx, next_idx, action, max_action, alpha, reward, gamma)\n",
    "        current_idx = next_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Task 2.2.4] Coding [10 points]\n",
    "Divide the 1000 traing episodes into 50 sets. Plot the average reward for each set. (i.e. plot the average reward for the first 20 episodes, the second 20 episodes, ..., and the 50th 20 episodes.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
