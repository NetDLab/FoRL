{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpQ-mdjhW6hd"
      },
      "source": [
        "<hr/>\n",
        "\n",
        "# EN.520.637 Foundations of Reinforcement Learning\n",
        "\n",
        "<hr/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFImeAHeW6hi"
      },
      "source": [
        "<h1><font color=\"darkblue\">Lab 4: Multi-armed Bandits, Monte Carlo Control & TD-Methods (60 points)  </font></h1>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##  Content\n",
        "1. Multi-armed Bandits\n",
        "2. Monte Carlo Methods\n",
        "3. TD-Methods: SARSA, Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFQSvOMQW6hj",
        "outputId": "8a670ad4-cbd0-4bd6-eb70-99ec2a462e69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "%pylab inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mtick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "NhzeZxOSW6hk"
      },
      "source": [
        "# Problem 1.  Greedy and $\\epsilon$-greedy algorithm (30 points)\n",
        "\n",
        "Taken from [P 21, Ch 2.3, Sutton&Barton]\n",
        "#### Define a 10-armed bandit problem in which the action values $q_*(a)$, $a = 1,...,10$, are samples from a standard normal distribution, i.e. Gaussian distribution with mean $= 0$ and variance $ = 1$. Then, when selected $A_t$ at time step $t$, the actual reward, $R_t$ is selected from a Gaussian distribution with mean = $q_*(A_t)$ and variance = 1.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkDaARWnW6hk"
      },
      "source": [
        "## 1.1 Epsilon greedy (15 points)\n",
        "\n",
        "We will see the average performance of epsilon greedy algorithms on this problem. To this matter, you will:\n",
        "- given the bandits, use your algorithm for 1000 time steps.\n",
        "- Repeat for 2000 independent trials.\n",
        "\n",
        "1. Implement a function/functions that runs 2000 independent experiments. An experiment is defined as 1. an initialization of the bandit reward distributions, and 2. running your algorithm for 1000 time steps, using an $\\epsilon$-greedy algorithm. Your function/functions should take $\\epsilon$ as one of the inputs and output:\n",
        "<br>   (a) average reward at each time step\n",
        "<br>   (b) percentage of optimal action at each time step. (optimal action is defined by $a^* = arg\\underset{a}max           \\, q^*(a)$ )\n",
        "3. Call your function/functions to generate the average reward and percentage of optimal action at each time step with:\n",
        "<br>   (a) Greedy-algorithm\n",
        "<br>   (b) $\\epsilon$-greedy algorithm, $\\epsilon=0.01$\n",
        "<br>   (c) $\\epsilon$-greedy algorithm, $\\epsilon=0.1$.\n",
        "4. Plot the average reward and percentage of optimal action of those three cases and compare with [P 23 Fig 2.2 Sutton]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQFEzO3DW6hl"
      },
      "source": [
        "### 1.1.1 (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdmO2GTkW6hl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tDL2ZXuW6hl"
      },
      "source": [
        "### 1.1.2 (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Fwh6PPGW6hm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wbdsiZUW6hm"
      },
      "source": [
        "### 1.1.3 (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_nSi5reW6hm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8I4ufuMLW6hn"
      },
      "source": [
        "---\n",
        "## 1.2  UCB Action Selection (15 points)\n",
        "\n",
        "1. Implement a function/functions that runs 2000 instances of this experiment, each experiment for 1000 time steps, using UCB Action Selection algorithm. Your function/functions should take $c$ as one of the inputs and output:\n",
        "<br>   - average reward at each time step.\n",
        "2. Call your function/functions to generate the average reward at each time step with:\n",
        "<br>   - UCB Action Selection algorithm, $c = 2$.\n",
        "3. Plot the average reward of 2.2 and 1.2c, then compare with [P 28 Fig 2.4 Sutton]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZDJAc-1W6hn"
      },
      "source": [
        "### 1.2.1 (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy59P_jJW6ho"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djl48fAEW6ho"
      },
      "source": [
        "### 1.2.2 (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd9y8IsNW6hp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jqz79rNW6hp"
      },
      "source": [
        "### 1.2.3 (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cmw_-mnCW6hp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "M3gCHzhqW6hp"
      },
      "source": [
        "# Problem 2. Monte Carlo Methods  (CartPole-v1 environment) (20 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "nmJOiLGAW6hq"
      },
      "source": [
        "## 2.1 CartPole Introduction\n",
        "\n",
        "We now apply Monte Carlo Method for the CartPole problem. See:\n",
        "[CartPole documentation](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
        "\n",
        "\n",
        "1. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track.\n",
        "\n",
        "0. The system is controlled by applying a force of +1 or -1 to the cart.\n",
        "\n",
        "0. The pendulum starts upright, and the goal is to prevent it from falling over.\n",
        "\n",
        "0. A reward of +1 is provided for every timestep that the pole remains upright.\n",
        "\n",
        "0. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
        "\n",
        "0. For more info (See [SOURCE ON GITHUB](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)).\n",
        "\n",
        "\n",
        "\n",
        "The following examples show the basic usage of this testing environment:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvLVNkC7W6hq"
      },
      "source": [
        "### 2.1.1 Episode initialization and Initial Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05aBMjR3W6hq"
      },
      "outputs": [],
      "source": [
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6mbQNzhW6hr",
        "outputId": "74b6ff5e-e14e-4bd2-deb3-7bdec3ee753a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inital observation is [0.00821616 0.03603547 0.03568365 0.01848931]\n",
            "\n",
            "This means the cart current position is 0.008216163143515587 with velocity 0.03603547438979149,\n",
            "and the pole current angular position is 0.03568364679813385 with angular velocity 0.018489308655261993,\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/agu/opt/anaconda3/envs/safe-gym/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "observation = env.reset() ##Initial an episode\n",
        "\n",
        "if gym.__version__>'0.26.0':\n",
        "    observation = observation[0]\n",
        "\n",
        "print(\"Inital observation is {}\".format(observation))\n",
        "\n",
        "print(\"\\nThis means the cart current position is {}\".format(observation[0]), end = '')\n",
        "print(\" with velocity {},\".format(observation[1]))\n",
        "\n",
        "print(\"and the pole current angular position is {}\".format(observation[2]), end = '')\n",
        "print(\" with angular velocity {},\".format(observation[3]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q303dJtW6hs"
      },
      "source": [
        "### 2.1.2 Take actions\n",
        "\n",
        "\n",
        "Use env.step(action) to take an action\n",
        "\n",
        "action is an integer from 0 to 1\n",
        "\n",
        "0: \"Left\"; 1: \"Right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X32cjNKkW6hs",
        "outputId": "3e26380f-8308-4c2c-a2b7-d32740e49454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current observation is [0.00821616 0.03603547 0.03568365 0.01848931]\n",
            "\n",
            "New observation is [ 0.00893687 -0.15957958  0.03605343  0.32221386]\n",
            "Step reward is 1.0\n",
            "Did episode just end? -False\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/agu/opt/anaconda3/envs/safe-gym/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "print(\"Current observation is {}\".format(observation))\n",
        "\n",
        "action = 0 #go left\n",
        "\n",
        "#################### simulate one step\n",
        "if gym.__version__>'0.26.0':\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "else:\n",
        "    observation, reward, done, info = env.step(action)\n",
        "####################\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nNew observation is {}\".format(observation))\n",
        "print(\"Step reward is {}\".format(reward))\n",
        "print(\"Did episode just end? -{}\".format(done)) # episode ends when 3.1(6) happens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5pq_YBOW6hs"
      },
      "source": [
        "### 2.1.3 Simulate multiple episodes\n",
        "\n",
        "(You may uncomment those lines to see an animation. However, it will not work for JupyterHub since the animation requires GL instead of webGL. If you have Jupyter notebook localy on your computer, this version of code will work through a virtual frame.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9yet6_5W6hs",
        "outputId": "d2c6ce3a-2473-428b-a060-85755411fe19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average reward per episode is 23.80952380952381\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "observation = env.reset()\n",
        "total_reward = 0\n",
        "ep_num = 0\n",
        "\n",
        "\n",
        "for _ in range(1000):\n",
        "\n",
        "    action = env.action_space.sample()     # this takes random actions\n",
        "\n",
        "    #################### simulate one step\n",
        "    if gym.__version__>'0.26.0':\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "    else:\n",
        "        observation, reward, done, info = env.step(action)\n",
        "    ####################\n",
        "\n",
        "    total_reward += reward\n",
        "\n",
        "\n",
        "\n",
        "    if done:                               # episode just ends\n",
        "        observation = env.reset()          # reset episode\n",
        "        ep_num += 1\n",
        "\n",
        "print(\"Average reward per episode is {}\".format(total_reward/ep_num))\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha2zEp3VW6ht"
      },
      "source": [
        "### 2.1.4 States Discretization\n",
        "\n",
        "The class DiscreteObs() discretizes the observation space into discrete state space, based on numpy.digitize (Please read its description in https://numpy.org/doc/stable/reference/generated/numpy.digitize.html)\n",
        "\n",
        "Discretization of observation space is necessary for tabular methods. You can use DiscreteObs() or any other library for discretizing the observation space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FjHINHcW6ht",
        "outputId": "ad72e6c2-ae1c-46a9-e0b6-8484e5cb09e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current position of the cart is 0.0187\n",
            "\n",
            "Current velocity of the cart is 0.0399\n",
            "\n",
            "Current angular position of the pole is -0.0320 rad\n",
            "\n",
            "Current angular velocity of the pole is -0.0040 rad\n",
            "\n",
            "which are mapped to state [20, 3, 18, 2], with corresponding index 24743\n"
          ]
        }
      ],
      "source": [
        "class DiscretObs():\n",
        "\n",
        "\n",
        "    def __init__(self, bins_list):\n",
        "        self._bins_list = bins_list\n",
        "\n",
        "        self._bins_num = len(bins_list)\n",
        "        self._state_num_list = [len(bins)+1 for bins in bins_list]\n",
        "        self._state_num_total = np.prod(self._state_num_list)\n",
        "\n",
        "    def get_state_num_total(self):\n",
        "\n",
        "        return self._state_num_total\n",
        "\n",
        "    def obs2state(self, obs):\n",
        "\n",
        "        if not len(obs)==self._bins_num:\n",
        "            raise ValueError(\"observation must have length {}\".format(self._bins_num))\n",
        "        else:\n",
        "            return [np.digitize(obs[i], bins=self._bins_list[i]) for i in range(self._bins_num)]\n",
        "\n",
        "    def obs2idx(self, obs):\n",
        "\n",
        "        state = self.obs2state(obs)\n",
        "\n",
        "        return self.state2idx(state)\n",
        "\n",
        "    def state2idx(self, state):\n",
        "\n",
        "        idx = 0\n",
        "        for i in range(self._bins_num-1,-1,-1):\n",
        "            idx = idx*self._state_num_list[i]+state[i]\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def idx2state(self, idx):\n",
        "\n",
        "        state = [None]*self._bins_num\n",
        "        state_num_cumul = np.cumprod(self._state_num_list)\n",
        "        for i in range(self._bins_num-1,0,-1):\n",
        "            state[i] = idx/state_num_cumul[i-1]\n",
        "            idx -=state[i]*state_num_cumul[i-1]\n",
        "        state[0] = idx%state_num_cumul[0]\n",
        "\n",
        "        return state\n",
        "\n",
        "# Recommended Discretization for Carpole-v1 when using Monte-Carlo methods\n",
        "bins_pos = np.linspace(-2.4,2.4,40)        # position\n",
        "bins_d_pos = np.linspace(-3,3,5)           # velocity\n",
        "bins_ang = np.linspace(-0.2618,0.2618,40)  # angle\n",
        "bins_d_ang = np.linspace(-0.3,0.3,5)       # angular velocity\n",
        "\n",
        "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
        "observation = env.reset()\n",
        "if gym.__version__>'0.26.0':\n",
        "    observation = observation[0]\n",
        "\n",
        "state = dobs.obs2state(observation)\n",
        "idx = dobs.obs2idx(observation)\n",
        "\n",
        "print(\"Current position of the cart is {:.4f}\\n\".format(observation[0]))\n",
        "print(\"Current velocity of the cart is {:.4f}\\n\".format(observation[1]))\n",
        "print(\"Current angular position of the pole is {:.4f} rad\\n\".format(observation[2]))\n",
        "print(\"Current angular velocity of the pole is {:.4f} rad\\n\".format(observation[3]))\n",
        "\n",
        "print(\"which are mapped to state {}, with corresponding index {}\".format(state,idx))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEYqtuQkW6ht"
      },
      "source": [
        "## 2.2 On-policy first-visit MC control\n",
        "### Task 2. [Coding, 20 points]\n",
        "\n",
        "1. Implement \"On-policy first-visit MC control\" algorithum in [Ch 5.4 Sutton] to choose optimal actions\n",
        "2. Simulate this algorithm for 40000 episodes.\n",
        "3. Plot the reward for each episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5cbU1EsW6ht"
      },
      "source": [
        "### 2.1 Implementation (15 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNQyX_zJW6hu",
        "outputId": "b383df4a-ae54-44f4-84fe-a2eb5e1df74f"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (2946652859.py, line 51)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 51\u001b[0;36m\u001b[0m\n\u001b[0;31m    current_state =                             # discretize the observation space\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "## Suggested functions (Feel free to modify existing and add new functions)\n",
        "\n",
        "def get_action(current_state, Q, epsilon):\n",
        "\n",
        "    # Choose optimal action based on current state and Q\n",
        "    #\n",
        "    # input:  current state,  (array)\n",
        "    #         Q,              (array)\n",
        "    #         epsilonn,       (float)\n",
        "    # output: action\n",
        "    #\n",
        "    return action\n",
        "\n",
        "\n",
        "\n",
        "def update_Q(Q, observation_list, action_list):\n",
        "    # Update Q at the end of each episode\n",
        "    #\n",
        "    # input:  current Q, (array)\n",
        "    #         observation_list,       (array)  states observed in this episode\n",
        "    #         action_list,       (array)  actions took in this spisode\n",
        "    # output: Updated Q\n",
        "    #\n",
        "\n",
        "\n",
        "    return Q\n",
        "\n",
        "\n",
        "## Suggested flow (Feel free to modify and add)\n",
        "\n",
        "# parameters for epsilon-greedy algorithm, when epsilon_decay_rate=1, the algorothm implement a fixed\n",
        "# epsilon value as epsilon_start, you can choose either fixed epsilon or decaying epsilon\n",
        "\n",
        "# epsilon_start = 0.3\n",
        "# epsilon_decay_rate = 0.97\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "observation = env.reset()\n",
        "if gym.__version__>'0.26.0':\n",
        "    observation = observation[0]\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# Add all the code snippets that you need here\n",
        "# --------------------------------------------\n",
        "\n",
        "while 1:\n",
        "\n",
        "\n",
        "    current_state =                             # discretize the observation space\n",
        "\n",
        "    action = get_action(current_state,Q,epsilon)# pick action by epsilon greedy policy\n",
        "\n",
        "    #################### simulate one step\n",
        "    if gym.__version__>'0.26.0':\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "    else:\n",
        "        observation, reward, done, info = env.step(action)\n",
        "    ####################\n",
        "\n",
        "    if done:  # end of epsode\n",
        "        Q = update_Q(Q, observation_list, action_list) # update Q for past observations in the episode\n",
        "\n",
        "        observation = env.reset()\n",
        "        if gym.__version__>'0.26.0':\n",
        "            observation = observation[0]\n",
        "\n",
        "        ep_num += 1\n",
        "\n",
        "        if condition:\n",
        "            # Apply epsilon decay (if you want to. You can do this for example every N episodes.)\n",
        "            pass\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkHcmHCRW6hu"
      },
      "source": [
        "### 2.2 Plot your results (5 points)\n",
        "\n",
        "Since results are noisy, you can try:\n",
        "- Plotting your un-filtered results\n",
        "- using window-averaging to smooth out your results (code snippet below)\n",
        "\n",
        "Plot both curves in the same figure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TS1sNczUW6hu"
      },
      "outputs": [],
      "source": [
        "# Window averaging to smooth out plots\n",
        "\n",
        "def moving_average(arr, window_size):\n",
        "    # Smoothes array 'arr' with 'window size'\n",
        "    window = np.ones(window_size) / window_size\n",
        "    # Apply convolution for moving average\n",
        "    return np.convolve(arr, window, mode='valid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "bD9Bo3O-W6hu"
      },
      "outputs": [],
      "source": [
        "### Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2b-VuNNW6hu"
      },
      "source": [
        "# Problem 3: TD-Methods (SARSA & Q-learning) (60 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7y73lnW6hv"
      },
      "source": [
        "## 3.1 Cliff walk enviornment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wOxCi_wW6hv"
      },
      "source": [
        "### 3.1.1 Intro to Cliff walk\n",
        "\n",
        "In this section, we use SARSA and Q-learning algorithm to solve to a cliff walk problem. ( See Sutton&Barto Example 6.6 )\n",
        "\n",
        "The grid is shown below, the black tiles represents wall/obstacles, the white tiles are the non-terminal tiles, and the tile with \"s\" is the starting point of every episoid, the tile with \"G\" is the goal point.\n",
        "\n",
        "The agent start at \"s\" tile. At every step, the agent can choose one of the four actions:\"up\",\"right\",\"down\",\"left\", moving to the next tile in that direction.\n",
        "\n",
        "$\\cdot$ If the next tile is wall/obstacle, the agent does not move and receive -1 reward;\n",
        "\n",
        "$\\cdot$ If the next tile is a non-terminal tile, the agent move to that tile and receive 0 reward;\n",
        "\n",
        "$\\cdot$ If the next tile is the goal tile, the episoid is finished and the agent receive 100 reward (set to be 100 to accelarate the training).\n",
        "\n",
        "$\\cdot$ If the next tile is the cliff, the episoid is finished and the agent receive -100 reward ;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The environment\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CliffWalk:\n",
        "\n",
        "    def __init__(self, reward_wall=-1):\n",
        "        # initialize grid with 2d numpy array\n",
        "        # >0: goal\n",
        "        # -1: wall/obstacles\n",
        "        # 0: non-terminal\n",
        "        self._grid = np.array(\n",
        "            [ [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "             [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "             [0, 0, 0, 0, 0, 0, 0, 0],\n",
        "             [0, -100, -100, -100, -100, -100, -100, 100]\n",
        "             ])\n",
        "        # wall around the grid, padding grid with -1\n",
        "        self._grid_padded = np.pad(self._grid, pad_width=1, mode='constant', constant_values=-1)\n",
        "        self._reward_wall = reward_wall\n",
        "\n",
        "        # set start state\n",
        "        self._start_state = (3, 0)\n",
        "        self._random_start = False\n",
        "\n",
        "        # store position of goal states and non-terminal states\n",
        "        idx_goal_state_y, idx_goal_state_x = np.nonzero(self._grid > 0)\n",
        "        self._goal_states = [(idx_goal_state_y[i], idx_goal_state_x[i]) for i in range(len(idx_goal_state_x))]\n",
        "\n",
        "        idx_non_term_y, idx_non_term_x = np.nonzero(self._grid == 0)\n",
        "        self._non_term_states = [(idx_non_term_y[i], idx_non_term_x[i]) for i in range(len(idx_non_term_x))]\n",
        "\n",
        "        # store the current state in the padded grid\n",
        "        self._state_padded = (self._start_state[0] + 1, self._start_state[1] + 1)\n",
        "\n",
        "    def get_state_num(self):\n",
        "        # get the number of states (total_state_number) in the grid, note: the wall/obstacles inside the grid are\n",
        "        # counted as state as well\n",
        "        return np.prod(np.shape(self._grid))\n",
        "\n",
        "    def get_state_grid(self):\n",
        "\n",
        "        state_grid = np.multiply(np.reshape(np.arange(self.get_state_num()), self._grid.shape), self._grid >= 0) - (\n",
        "                self._grid == -1) #- (self._grid == -100)\n",
        "\n",
        "        return state_grid, np.pad(state_grid, pad_width=1, mode='constant', constant_values=-1)\n",
        "\n",
        "    def get_current_state(self):\n",
        "        # get the current state as an integer from 0 to total_state_number-1\n",
        "        y, x = self._state_padded\n",
        "        return (y - 1) * self._grid.shape[1] + (x - 1)\n",
        "\n",
        "    def int_to_state(self, int_obs):\n",
        "        # convert an integer from 0 to total_state_number-1 to the position on the non-padded grid\n",
        "        x = int_obs % self._grid.shape[1]\n",
        "        y = int_obs // self._grid.shape[1]\n",
        "        return y, x\n",
        "\n",
        "    def reset(self):\n",
        "        # reset the gridworld\n",
        "        if self._random_start:\n",
        "            # randomly start at a non-terminal state\n",
        "            idx_start = np.random.randint(len(self._non_term_states))\n",
        "            start_state = self._non_term_states[idx_start]\n",
        "            self._state_padded = (start_state[0] + 1, start_state[1] + 1)\n",
        "        else:\n",
        "            # start at the designated start_state\n",
        "            self._state_padded = (self._start_state[0] + 1, self._start_state[1] + 1)\n",
        "\n",
        "    def step(self, action):\n",
        "        # take one step according to the action\n",
        "        # input: action (integer between 0 and 3)\n",
        "        # output: reward           reward of this action\n",
        "        #         terminated       1 if reaching the terminal state, 0 otherwise\n",
        "        #         next_state       next state after this action, integer from 0 to total_state_number-1)\n",
        "        y, x = self._state_padded\n",
        "\n",
        "        if action == 0:  # up\n",
        "            new_state_padded = (y - 1, x)\n",
        "        elif action == 1:  # right\n",
        "            new_state_padded = (y, x + 1)\n",
        "        elif action == 2:  # down\n",
        "            new_state_padded = (y + 1, x)\n",
        "        elif action == 3:  # left\n",
        "            new_state_padded = (y, x - 1)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action: {} is not 0, 1, 2, or 3.\".format(action))\n",
        "\n",
        "        new_y, new_x = new_state_padded\n",
        "        if self._grid_padded[new_y, new_x] == -1:  # wall/obstacle\n",
        "            reward = self._reward_wall\n",
        "            new_state_padded = (y, x)\n",
        "        elif self._grid_padded[new_y, new_x] == 0:  # non-terminal cell\n",
        "            reward = -1.\n",
        "        else:  # a goal\n",
        "            reward = self._grid_padded[new_y, new_x]\n",
        "            self.reset()\n",
        "            terminated = 1\n",
        "            return reward, terminated, self.get_current_state()\n",
        "\n",
        "        terminated = 0\n",
        "        self._state_padded = new_state_padded\n",
        "        return reward, terminated, self.get_current_state()\n",
        "\n",
        "\n",
        "    def plot_grid(self, plot_title=None):\n",
        "        # plot the grid\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow((self._grid_padded == -1) + (self._grid_padded == -100) * 0.5, cmap='Greys', vmin=0, vmax=1)\n",
        "        ax = plt.gca()\n",
        "        ax.grid(0)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "        if plot_title:\n",
        "            plt.title(plot_title)\n",
        "\n",
        "        plt.text(\n",
        "            self._start_state[1] + 1, self._start_state[0] + 1,\n",
        "            r\"$\\mathbf{S}$\", ha='center', va='center')\n",
        "        for goal_state in self._goal_states:\n",
        "            plt.text(\n",
        "                goal_state[1] + 1, goal_state[0] + 1,\n",
        "                r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "        h, w = self._grid_padded.shape\n",
        "        for y in range(h - 1):\n",
        "            plt.plot([-0.5, w - 0.5], [y + 0.5, y + 0.5], '-k', lw=2)\n",
        "        for x in range(w - 1):\n",
        "            if x in np.arange(2,7):\n",
        "                plt.plot([x + 0.5, x + 0.5], [-0.5, h - 2.5], '-k', lw=2)\n",
        "                continue\n",
        "            plt.plot([x + 0.5, x + 0.5], [-0.5, h - 0.5], '-k', lw=2)\n",
        "        plt.text(4.5, 4,r\"T h e   C l i f f\", ha='center', va='center')\n",
        "\n",
        "    def plot_state_values(self, state_values, value_format=\"{:.1f}\",plot_title=None):\n",
        "        # plot the state values\n",
        "        # input: state_values     (total_state_number, )-numpy array, state value function\n",
        "        #        plot_title       str, title of the plot\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow((self._grid_padded == -1) + (self._grid_padded == -100) * 0.5, cmap='Greys', vmin=0, vmax=1)\n",
        "        ax = plt.gca()\n",
        "        ax.grid(0)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "        if plot_title:\n",
        "            plt.title(plot_title)\n",
        "\n",
        "        for (int_obs, state_value) in enumerate(state_values):\n",
        "            y, x = self.int_to_state(int_obs)\n",
        "            if (y, x) in self._non_term_states:\n",
        "                plt.text(x + 1, y + 1, value_format.format(state_value), ha='center', va='center')\n",
        "        for goal_state in self._goal_states:\n",
        "            plt.text(\n",
        "                goal_state[1] + 1, goal_state[0] + 1,\n",
        "                r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "        h, w = self._grid_padded.shape\n",
        "        for y in range(h - 1):\n",
        "            plt.plot([-0.5, w - 0.5], [y + 0.5, y + 0.5], '-k', lw=2)\n",
        "        for x in range(w - 1):\n",
        "            if x in np.arange(2,7):\n",
        "                plt.plot([x + 0.5, x + 0.5], [-0.5, h - 2.5], '-k', lw=2)\n",
        "                continue\n",
        "            plt.plot([x + 0.5, x + 0.5], [-0.5, h - 0.5], '-k', lw=2)\n",
        "\n",
        "    def plot_policy(self, policy, plot_title=None):\n",
        "        # plot a deterministic policy\n",
        "        # input: policy           (total_state_number, )-numpy array, contains action as integer from 0 to 3\n",
        "        #        plot_title       str, title of the plot\n",
        "        action_names = [r\"$\\uparrow$\", r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow((self._grid_padded == -1) + (self._grid_padded == -100) * 0.5, cmap='Greys', vmin=0, vmax=1)\n",
        "        ax = plt.gca()\n",
        "        ax.grid(0)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "\n",
        "        for goal_state in self._goal_states:\n",
        "            plt.text(\n",
        "                goal_state[1] + 1, goal_state[0] + 1,\n",
        "                r\"$\\mathbf{G}$\", ha='center', va='center')\n",
        "\n",
        "        if plot_title:\n",
        "            plt.title(plot_title)\n",
        "\n",
        "        for (int_obs, action) in enumerate(policy):\n",
        "            y, x = self.int_to_state(int_obs)\n",
        "            if (y, x) in self._non_term_states:\n",
        "                action_arrow = action_names[action]\n",
        "                plt.text(x + 1, y + 1, action_arrow, ha='center', va='center')\n"
      ],
      "metadata": {
        "id": "FzBdALilXcoN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "_3c1960mW6hv",
        "outputId": "5075547a-a7d6-4a3f-cc65-e4b2e5f0f517"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "gw = CliffWalk()\n",
        "gw.plot_grid(plot_title='The grid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBXwHzHFW6hv"
      },
      "source": [
        "### 3.1.2 States and state values\n",
        "Excluding the wall around the grid, there are 32 tiles (INCLUDING obstacles inside the grid), and they correspond to 32 states (obstacles and goal are non-reachable states).\n",
        "\n",
        "We use numbers from 0 to 24 to represent these states (see gridworld.py for the coversion between integer and tile position). The correspondance are as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "CgUoxAvrW6hw",
        "outputId": "f9dcf5a9-7b26-4695-8fb3-6f147d0613aa"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "gw.plot_state_values(np.arange(25),value_format=\"{:d}\",plot_title='Each state as an integer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWBqNYmkW6hw"
      },
      "source": [
        "### 3.1.3 Taking actions\n",
        "Use GridWorld.step(action) to take an action, and use GridWorld.reset() to restart an episoid\n",
        "\n",
        "action is an integer from 0 to 3\n",
        "\n",
        "0: \"Up\"; 1: \"Right\"; 2: \"Down\"; 3: \"Left\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95o9LfLjW6hw"
      },
      "outputs": [],
      "source": [
        "gw.reset()\n",
        "\n",
        "current_state = gw.get_current_state()\n",
        "tile_pos = gw.int_to_state(current_state)\n",
        "\n",
        "print(\"The current state is {}, which corresponds to tile position {}\\n\".format(current_state,tile_pos))\n",
        "\n",
        "action = np.random.randint(4)\n",
        "reward, terminated, next_state = gw.step(action)\n",
        "tile_pos = gw.int_to_state(next_state)\n",
        "\n",
        "print(\"Take action {}, get reward {}, move to state {}\".format(action,reward,next_state))\n",
        "print(\"Now the current state is {}, which corresponds to tile position {}\\n\".format(next_state,tile_pos))\n",
        "\n",
        "gw.reset()\n",
        "current_state = gw.get_current_state()\n",
        "tile_pos = gw.int_to_state(current_state)\n",
        "print(\"Reset episode\")\n",
        "print(\"Now the current state is {}, which corresponds to tile position {}\".format(current_state,tile_pos))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUWz8UjpW6hw"
      },
      "source": [
        "### 3.1.4. Plotting Deterministic Policies\n",
        "A deterministic policy is a function from state to action, which can be represented by a (32,)-numpy array whose entries are all integers in (0-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAH_Dt8WW6hw"
      },
      "outputs": [],
      "source": [
        "gw.plot_policy(np.random.randint(4,size=(32,)),plot_title='A deterministic policy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZn-Gn-ZW6hw"
      },
      "source": [
        "## 3.2 SARSA & Q_learning\n",
        "\n",
        "You will now implement Sarsa and Q learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVREriPaW6hw"
      },
      "outputs": [],
      "source": [
        "## Suggested functions (Feel free to modify existing and add new functions)\n",
        "\n",
        "\n",
        "def update_Q(Q, current_idx, next_idx, current_action, next_action, alpha, R, gamma):\n",
        "    # Update Q at the each step\n",
        "    #\n",
        "    # input:  current Q,                    (array)\n",
        "    #         current_idx, next_idx         (array)  states\n",
        "    #         current_action, next_action   (array)  actions\n",
        "    #         alpha, R, gamma               (floats) learning rate, reward, discount rate\n",
        "    # output: Updated Q\n",
        "    #\n",
        "    return Q\n",
        "\n",
        "def get_action(current_idx, Q, epsilon):\n",
        "\n",
        "    # Choose optimal action based on current state and Q\n",
        "    #\n",
        "    # input:  current_idx     (array)\n",
        "    #         Q,              (array)\n",
        "    #         epsilon,        (float)\n",
        "    # output: action\n",
        "    return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WHbqecaW6hx"
      },
      "source": [
        "### [Task] 3.2.1 SARSA [Coding: 10 points]\n",
        "\n",
        "* Implement SARSA (See Sutton&Barto Section 6.4) on this example for 5000 episodes to learn the optimal policy.\n",
        "* Plot the greedy policy of the learned Q-function using gw.plot_policy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fABpjpFoW6hx"
      },
      "outputs": [],
      "source": [
        "## Suggested flow (Feel free to modify and add)\n",
        "## SARSA\n",
        "Q  = np.zeros((25,4))\n",
        "\n",
        "gw.reset()\n",
        "\n",
        "max_ep = 5000\n",
        "\n",
        "total_reward_sarsa = np.zeros(max_ep)\n",
        "\n",
        "epsilon = 0.1\n",
        "alpha = 0.5\n",
        "gamma = 0.9\n",
        "\n",
        "\n",
        "for ep in range(0, max_ep):\n",
        "    gw.reset()\n",
        "    terminated = False\n",
        "\n",
        "\n",
        "\n",
        "    while terminated == False:\n",
        "        reward, terminated, next_state = gw.step(current_action)\n",
        "        if not reward == 100: total_reward_sarsa[ep] += reward\n",
        "        next_action = get_action(next_state,Q,epsilon)\n",
        "\n",
        "        Q = update_Q(Q, current_state, next_state, current_action, next_action, alpha, reward, gamma)\n",
        "\n",
        "\n",
        "        current_state = next_state\n",
        "        current_action = next_action\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQ6JMrc_W6hx"
      },
      "source": [
        "### [Task] 3.2.2 Q-learning [Coding: 10 points]\n",
        "* Implement Q_learning algorithm (See Sutton&Barto Section 6.5) on this example for 5000 episodes to learn the optimal policy.\n",
        "* Plot the greedy policy of the learned Q-function using gw.plot_policy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaeRZkZmW6hx"
      },
      "outputs": [],
      "source": [
        "## Suggested flow (Feel free to modify and add)\n",
        "## Q_learning\n",
        "Q  = np.zeros((25,4))\n",
        "\n",
        "\n",
        "gw.reset()\n",
        "\n",
        "\n",
        "max_ep = 5000\n",
        "\n",
        "total_reward_qlearning = np.zeros(max_ep)\n",
        "\n",
        "epsilon = 0.1\n",
        "alpha = 0.5\n",
        "gamma = 0.9\n",
        "\n",
        "\n",
        "for ep in range(0, max_ep):\n",
        "    gw.reset()\n",
        "    terminated = False\n",
        "\n",
        "\n",
        "\n",
        "    while terminated == False:\n",
        "        reward, terminated, next_state = gw.step(current_action)\n",
        "        if not reward == 100: total_reward_qlearning[ep] += reward\n",
        "        max_action = get_action(next_state,Q,0)\n",
        "\n",
        "\n",
        "        Q = update_Q(Q, current_state, next_state, current_action, max_action, alpha, reward, gamma)\n",
        "\n",
        "\n",
        "        current_state = next_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xpCsLIvW6hx"
      },
      "source": [
        "### [Task 3.2.3] Comparison [Coding/Question 10 points]\n",
        "* Plot the total rewards during one episode v.s. number of episodes trained for both SARSA and Q-Learning.\n",
        "\n",
        "* Compare your plot to the one in [Sutton & Barto Figure 6.4].\n",
        "\n",
        "* Which algorithm obtains bette performance? Provide some intuition on why this is the case.\n",
        "\n",
        "(Optional)You may  \n",
        "\n",
        "1. Smooth your curve by taking the average of total rewards over successive 50 episodes\n",
        "2. Avoid adding the artificial \"+100\" goal reward to the total reward to match you figure with the book (Although we need to used goal reward when update the Q-function )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUCH4y8-W6hx"
      },
      "outputs": [],
      "source": [
        "# Your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wstMCE-4W6hx"
      },
      "source": [
        "***Your written answer/comparison here***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7YaQJs7W6hy"
      },
      "source": [
        "## 3.3 CartPole-v1 environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmhEcTG0W6hy"
      },
      "source": [
        "### 3.3.1 CartPole Introduction\n",
        "\n",
        "We now use SARSA and Q-learning on the CartPole problem.\n",
        "\n",
        "\n",
        "1. A pole is attached via an un-actuated joint to a cart, which moves along a frictionless track.\n",
        "\n",
        "0. The system is controlled by applying a force of +1 or -1 to the cart.\n",
        "\n",
        "0. The pole starts at upright position, and the goal is to prevent it from falling over.\n",
        "\n",
        "0. A reward of +1 is obtained for every timestep that the pole remains upright.\n",
        "\n",
        "0. The episode ends when the pole is more than 15 degrees from the vertical, or the cart moves more than 2.4 units from the center.\n",
        "\n",
        "0. For more info (See [SOURCE ON GITHUB](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)).\n",
        "\n",
        "The following examples show the basic usage of this testing environment:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SYA1EinW6hy"
      },
      "source": [
        "### Episode initialization and Initial Value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC7fEXkHW6hy"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "observation = env.reset() ##Initial an episode\n",
        "if gym.__version__>'0.26.0':\n",
        "    observation = observation[0]\n",
        "\n",
        "print(\"Inital observation is {}\".format(observation))\n",
        "\n",
        "print(\"\\nThis means the cart current position is {}\".format(observation[0]), end = '')\n",
        "print(\" with velocity {},\".format(observation[1]))\n",
        "\n",
        "print(\"and the pole current angular position is {}\".format(observation[2]), end = '')\n",
        "print(\" with angular velocity {},\".format(observation[3]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_rs4oK-W6hy"
      },
      "source": [
        "### Taking actions\n",
        "\n",
        "\n",
        "Use env.step(action) to take an action\n",
        "\n",
        "action is an integer from 0 to 1\n",
        "\n",
        "0: \"Left\"; 1: \"Right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHWCPqCkW6hy"
      },
      "outputs": [],
      "source": [
        "print(\"Current observation is {}\".format(observation))\n",
        "\n",
        "action = 0 #go left\n",
        "#################### simulate one step\n",
        "if gym.__version__>'0.26.0':\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "else:\n",
        "    observation, reward, done, info = env.step(action)\n",
        "####################\n",
        "\n",
        "print(\"\\nNew observation is {}\".format(observation))\n",
        "print(\"Step reward is {}\".format(reward))\n",
        "print(\"Did episode just ends? -{}\".format(done)) # episode ends when 3.1(6) happens\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YphCV7C7W6hy"
      },
      "source": [
        "### Simulating multiple episodes\n",
        "\n",
        "(You may uncomment those lines to see an animation. However, it will not work for JupyterHub since the animation requires GL instead of webGL. If you have Jupyter notebook localy on your computer, this version of code will work through a virtual frame.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wbJPGOtW6hy"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v0')\n",
        "observation = env.reset()\n",
        "if gym.__version__>'0.26.0':\n",
        "    observation = observation[0]\n",
        "total_reward = 0\n",
        "ep_num = 0\n",
        "# img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "\n",
        "for _ in range(1000):\n",
        "    #     img.set_data(env.render(mode='rgb_array'))\n",
        "    #     display.display(plt.gcf())\n",
        "    #     display.clear_output(wait=True)\n",
        "\n",
        "    action = env.action_space.sample()     # this takes random actions\n",
        "    #################### simulate one step\n",
        "    if gym.__version__>'0.26.0':\n",
        "        observation, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "    else:\n",
        "        observation, reward, done, info = env.step(action)\n",
        "    ####################\n",
        "\n",
        "    total_reward += reward\n",
        "\n",
        "\n",
        "\n",
        "    if done:                               # episode just ends\n",
        "        observation = env.reset()          # reset episode\n",
        "        if gym.__version__>'0.26.0':\n",
        "            observation = observation[0]\n",
        "        ep_num += 1\n",
        "\n",
        "print(\"Average reward per episode is {}\".format(total_reward/ep_num))\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ59PFZTW6hz"
      },
      "source": [
        "### States Discretization\n",
        "\n",
        "The class DiscreteObs() discretizes the observation space into discrete state space, based on numpy.digitize (Please read its description in https://numpy.org/doc/stable/reference/generated/numpy.digitize.html)\n",
        "\n",
        "Discretization of observation space is necessary for tabular methods. You can use DiscreteObs() or any other library for discretizing the observation space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M5_X59nW6hz"
      },
      "outputs": [],
      "source": [
        "class DiscretObs():\n",
        "\n",
        "\n",
        "    def __init__(self, bins_list):\n",
        "        self._bins_list = bins_list\n",
        "\n",
        "        self._bins_num = len(bins_list)\n",
        "        self._state_num_list = [len(bins)+1 for bins in bins_list]\n",
        "        self._state_num_total = np.prod(self._state_num_list)\n",
        "\n",
        "    def get_state_num_total(self):\n",
        "\n",
        "        return self._state_num_total\n",
        "\n",
        "    def _state_num_list(self):\n",
        "\n",
        "        return self._state_num_list\n",
        "\n",
        "    def obs2state(self, obs):\n",
        "\n",
        "        if not len(obs)==self._bins_num:\n",
        "            raise ValueError(\"observation must have length {}\".format(self._bins_num))\n",
        "        else:\n",
        "            return [np.digitize(obs[i], bins=self._bins_list[i]) for i in range(self._bins_num)]\n",
        "\n",
        "    def obs2idx(self, obs):\n",
        "\n",
        "        state = self.obs2state(obs)\n",
        "\n",
        "        return self.state2idx(state)\n",
        "\n",
        "    def state2idx(self, state):\n",
        "\n",
        "        idx = 0\n",
        "        for i in range(self._bins_num-1,-1,-1):\n",
        "            idx = idx*self._state_num_list[i]+state[i]\n",
        "\n",
        "        return idx\n",
        "\n",
        "    def idx2state(self, idx):\n",
        "\n",
        "        state = [None]*self._bins_num\n",
        "        state_num_cumul = np.cumprod(self._state_num_list)\n",
        "        for i in range(self._bins_num-1,0,-1):\n",
        "            state[i] = idx//state_num_cumul[i-1]\n",
        "            idx -=state[i]*state_num_cumul[i-1]\n",
        "        state[0] = idx%state_num_cumul[0]\n",
        "\n",
        "        return state\n",
        "\n",
        "# Recommended epsilon and learning_rate update (Feel free to modify existing and add new functions)\n",
        "def get_epsilon(t):\n",
        "    return max(0.1, min(1., 1. - math.log10((t + 1) / 25)))\n",
        "\n",
        "def get_learning_rate(t):\n",
        "    return max(0.1, min(1., 1. - math.log10((t + 1) / 25)))\n",
        "\n",
        "\n",
        "\n",
        "# Recommended Discretization for Carpole-v1 when using Monte-Carlo methods\n",
        "bins_pos = []                                       # position\n",
        "bins_d_pos = []                                     # velocity\n",
        "bins_ang = np.linspace(-0.41887903,0.41887903,5)    # angle\n",
        "bins_d_ang = np.linspace(-0.87266,0.87266,11)       # angular velocity\n",
        "\n",
        "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
        "observation = env.reset()\n",
        "if gym.__version__>'0.26.0':\n",
        "    observation = observation[0]\n",
        "\n",
        "state = dobs.obs2state(observation)\n",
        "\n",
        "idx = dobs.state2idx(state)\n",
        "\n",
        "\n",
        "print(\"Current position of the cart is {:.4f}\\n\".format(observation[0]))\n",
        "print(\"Current velocity of the cart is {:.4f}\\n\".format(observation[1]))\n",
        "print(\"Current angular position of the pole is {:.4f} rad\\n\".format(observation[2]))\n",
        "print(\"Current angular velocity of the pole is {:.4f} rad\\n\".format(observation[3]))\n",
        "\n",
        "print(\"which are mapped to state {}, with corresponding index {}\".format(state,idx))\n",
        "print(\"index {} maps to state{}\".format(idx,dobs.idx2state(idx)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9VrEm5hW6hz"
      },
      "source": [
        "## 3.4 SARSA & Q_learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oml3WzacW6hz"
      },
      "outputs": [],
      "source": [
        "## Suggested functions (Feel free to modify existing and add new functions)\n",
        "\n",
        "\n",
        "def update_Q(Q, current_idx, next_idx, current_action, next_action, alpha, R, gamma):\n",
        "    # Update Q at the each step\n",
        "    #\n",
        "    # input:  current Q,                    (array)\n",
        "    #         current_idx, next_idx         (array)  states\n",
        "    #         current_action, next_action   (array)  actions\n",
        "    #         alpha, R, gamma               (floats) learning rate, reward, discount rate\n",
        "    # output: Updated Q\n",
        "    #\n",
        "    return Q\n",
        "\n",
        "def get_action(current_idx, Q, epsilon):\n",
        "\n",
        "    # Choose optimal action based on current state and Q\n",
        "    #\n",
        "    # input:  current_idx     (array)\n",
        "    #         Q,              (array)\n",
        "    #         epsilon,        (float)\n",
        "    # output: action\n",
        "    return action\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6yAu_PYW6hz"
      },
      "source": [
        "### [Task 3.4.1] SARSA [Coding, 10 points]\n",
        "\n",
        "Implement SARSA algorithm (See Sutton&Barto Section 6.4) on this example for 1000 episodes to learn the optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TonBD8p9W6hz"
      },
      "outputs": [],
      "source": [
        "## Suggested flow (Feel free to modify and add)\n",
        "## SARSA\n",
        "total_reward = 0\n",
        "\n",
        "bins_pos = []\n",
        "bins_d_pos = []\n",
        "bins_ang = np.linspace(-0.41887903,0.41887903,5)\n",
        "bins_d_ang = np.linspace(-0.87266,0.87266,11)\n",
        "\n",
        "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "observation = env.reset()\n",
        "if gym.__version__>'0.26.0':\n",
        "    observation = observation[0]\n",
        "\n",
        "# Q defined by states\n",
        "# Q = np.zeros((2,dobs._state_num_list[0],dobs._state_num_list[1],dobs._state_num_list[2],dobs._state_num_list[3]))\n",
        "# Q defined by index\n",
        "# Q = np.zeros((2,dobs.get_state_num_total())\n",
        "\n",
        "count = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "gamma = 0.98\n",
        "result = np.zeros(50)\n",
        "s = 0\n",
        "for ep in range(1000):\n",
        "    if  np.mod(ep,20)==0:\n",
        "        result[s] = total_reward/20\n",
        "        s+=1\n",
        "        total_reward = 0\n",
        "\n",
        "    observation = env.reset()\n",
        "    if gym.__version__>'0.26.0':\n",
        "        observation = observation[0]\n",
        "\n",
        "#     current_state = dobs.obs2state(observation)\n",
        "#     current_idx = dobs.obs2idx(observation)\n",
        "\n",
        "    alpha = get_learning_rate(ep)\n",
        "    epsilon = get_epsilon(ep)\n",
        "\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        total_reward += 1\n",
        "        action =\n",
        "        #################### simulate one step\n",
        "        if gym.__version__>'0.26.0':\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "        else:\n",
        "            observation, reward, done, info = env.step(action)\n",
        "        ####################\n",
        "\n",
        "#         next_idx =\n",
        "#         next_state =\n",
        "        next_action =\n",
        "\n",
        "        Q = update_Q(Q, current_idx, next_idx, action, next_action, alpha, reward, gamma)\n",
        "        current_idx = next_idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXR7vmyUW6h0"
      },
      "source": [
        "### [Task 3.4.2] Coding [5 points]\n",
        "- Plot the average reward for each episode\n",
        "- You may (and are encouraged to) also plot the 'windowed' reward (you can use the same function from MonteCarlo method)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1wbvUz4W6h0"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAnCJEAjW6h0"
      },
      "source": [
        "### [Task 3.4.3] Q-learning [Coding, 10 points]\n",
        "\n",
        "Implement Q_learning algorithm (See Sutton&Barto Section 6.5) on this example for 1000 episodes to to learn the optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x43zG2uIW6h0"
      },
      "outputs": [],
      "source": [
        "## Suggested flow (Feel free to modify and add)\n",
        "## Q_learning\n",
        "total_reward = 0\n",
        "\n",
        "bins_pos = []\n",
        "bins_d_pos = []\n",
        "bins_ang = np.linspace(-0.41887903,0.41887903,5)\n",
        "bins_d_ang = np.linspace(-0.87266,0.87266,11)\n",
        "\n",
        "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "observation = env.reset()\n",
        "if gym.__version__>'0.26.0':\n",
        "    observation = observation[0]\n",
        "\n",
        "\n",
        "\n",
        "# Q defined by states\n",
        "# Q = np.zeros((2,dobs._state_num_list[0],dobs._state_num_list[1],dobs._state_num_list[2],dobs._state_num_list[3]))\n",
        "# Q defined by index\n",
        "# Q = np.zeros((2,dobs.get_state_num_total())\n",
        "\n",
        "gamma = 0.98\n",
        "result = np.zeros(50)\n",
        "s = 0\n",
        "for ep in range(1000):\n",
        "    if  np.mod(ep,20)==0:\n",
        "        result[s] = total_reward/20\n",
        "        s+=1\n",
        "        total_reward = 0\n",
        "\n",
        "    observation = env.reset()\n",
        "    if gym.__version__>'0.26.0':\n",
        "        observation = observation[0]\n",
        "\n",
        "#     current_state = dobs.obs2state(observation)\n",
        "#     current_idx = dobs.obs2idx(observation)\n",
        "\n",
        "    alpha = get_learning_rate(ep)\n",
        "    epsilon = get_epsilon(ep)\n",
        "\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        total_reward += 1\n",
        "        action =\n",
        "        #################### simulate one step\n",
        "        if gym.__version__>'0.26.0':\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "        else:\n",
        "            observation, reward, done, info = env.step(action)\n",
        "        ####################\n",
        "\n",
        "#         next_idx =\n",
        "#         nex_state =\n",
        "        max_action =\n",
        "\n",
        "        Q = update_Q(Q, current_idx, next_idx, action, max_action, alpha, reward, gamma)\n",
        "        current_idx = next_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFd83ycCW6h0"
      },
      "source": [
        "### [Task 3.4.3] Coding [5 points]\n",
        "- Plot the average reward for each episode\n",
        "- You may (and are encouraged to) also plot the 'windowed' reward (you can use the same function from MonteCarlo method)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBfW9HG4W6h0"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NhzeZxOSW6hk",
        "M3gCHzhqW6hp",
        "3wOxCi_wW6hv",
        "VBXwHzHFW6hv",
        "FWBqNYmkW6hw",
        "EUWz8UjpW6hw",
        "B7YaQJs7W6hy"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}