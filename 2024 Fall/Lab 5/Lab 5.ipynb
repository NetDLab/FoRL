{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5 : Policy gradients and REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will implement a neural network policy on the cartpole environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SYH5eZMDO9r",
    "outputId": "8d91703c-d5a9-464b-ad12-a98d185c2f83"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium\n",
    "!pip install torch\n",
    "!pip install 'typing-extensions>=4.3.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tYgCNLSJDTaL"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Helper code: Episode Logging\n",
    "# Book keeping of some useful (episode) statistics\n",
    "Logging_EpStats = namedtuple(\"Logging_EpStats\",\n",
    "                             [\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "# We are going to be storing transitions encountered in the episode to use\n",
    "# later in the update (at the end of the episode, for REINFORCE)\n",
    "Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"discount\"])\n",
    "\n",
    "def plot_stats(stats, window=10):\n",
    "  plt.figure(figsize=(16,4))\n",
    "  plt.subplot(121)\n",
    "  xline = range(0, len(stats.episode_lengths), window)\n",
    "  plt.plot(xline, smooth(stats.episode_lengths, window=window))\n",
    "  plt.ylabel('Episode Length')\n",
    "  plt.xlabel('Episode Count')\n",
    "  plt.subplot(122)\n",
    "  plt.plot(xline, smooth(stats.episode_rewards, window=window))\n",
    "  plt.ylabel('Episode Return')\n",
    "  plt.xlabel('Episode Count')\n",
    "\n",
    "def smooth(x, window=10):\n",
    "  return x[:window*(len(x)//window)].reshape(len(x)//window, window).mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### neural network helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eI_PJAwAFV-y"
   },
   "outputs": [],
   "source": [
    "def get_activation(\n",
    "    activation: str,\n",
    ") -> type[nn.Identity | nn.ReLU | nn.Sigmoid | nn.Softplus | nn.Tanh | nn.Softmax]:\n",
    "    \"\"\"Get the activation function.\n",
    "\n",
    "    The ``activation`` can be chosen from: ``identity``, ``relu``, ``sigmoid``, ``softplus``,\n",
    "    ``tanh``.\n",
    "\n",
    "    Args:\n",
    "        activation (Activation): The activation function.\n",
    "\n",
    "    Returns:\n",
    "        The activation function, ranging from ``nn.Identity``, ``nn.ReLU``, ``nn.Sigmoid``,\n",
    "        ``nn.Softplus`` to ``nn.Tanh``.\n",
    "    \"\"\"\n",
    "    activations = {\n",
    "        'identity': nn.Identity,\n",
    "        'relu': nn.ReLU,\n",
    "        'sigmoid': nn.Sigmoid,\n",
    "        'softplus': nn.Softplus,\n",
    "        'tanh': nn.Tanh,\n",
    "        'softmax': nn.Softmax\n",
    "    }\n",
    "    assert activation in activations\n",
    "    return activations[activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_Nvhyl4EzcZ"
   },
   "outputs": [],
   "source": [
    "def build_mlp_network(\n",
    "    sizes: list[int],\n",
    "    activation: str = 'tanh',\n",
    "    output_activation: str = 'identity',\n",
    ") -> nn.Module:\n",
    "\n",
    "    \"\"\"Build the MLP network.\n",
    "\n",
    "    Examples:\n",
    "        >>> build_mlp_network([64, 64, 64], 'relu', 'tanh')\n",
    "        Sequential(\n",
    "            (0): Linear(in_features=64, out_features=64, bias=True)\n",
    "            (1): ReLU()\n",
    "            (2): Linear(in_features=64, out_features=64, bias=True)\n",
    "            (3): ReLU()\n",
    "            (4): Linear(in_features=64, out_features=64, bias=True)\n",
    "            (5): Tanh()\n",
    "        )\n",
    "\n",
    "    Args:\n",
    "        sizes (list of int): The sizes of the layers.\n",
    "        activation (Activation): The activation function.\n",
    "        output_activation (Activation, optional): The output activation function. Defaults to\n",
    "            ``identity``.\n",
    "        weight_initialization_mode (InitFunction, optional): Weight initialization mode. Defaults to\n",
    "            ``'kaiming_uniform'``.\n",
    "\n",
    "    Returns:\n",
    "        The MLP network.\n",
    "    \"\"\"\n",
    "    activation_fn = get_activation(activation)\n",
    "    output_activation_fn = get_activation(output_activation)\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act_fn = activation_fn if j < len(sizes) - 2 else output_activation_fn\n",
    "        affine_layer = nn.Linear(sizes[j], sizes[j + 1])\n",
    "        # initialize_layer(weight_initialization_mode, affine_layer)\n",
    "        layers += [affine_layer, act_fn()]\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 [Derivation Task, 10 points] Gradient of softmax policy\n",
    "\n",
    "We will consider neural network policies given by arbitrary hidden layers and activations, and with a softmax layer at the end.\n",
    "\n",
    "In that way, our policy looks like:\n",
    "$$\\pi(a|s) = \\frac{\\exp(\\phi(s,a))}{\\sum_{a'\\in\\mathcal{A}}\\exp(\\phi(s,a'))},$$\n",
    "\n",
    "where $\\phi$ is the Neural Network (before the softmax activation.\n",
    "\n",
    "**Task:**\n",
    "For given (s,a), get a closed form solution for\n",
    "$$\\nabla\\log\\pi(a|s)$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 [Coding task, 20 points]\n",
    "Complete the highlighted methods for the PolicyApproximator class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1y70jXIIDZYZ"
   },
   "outputs": [],
   "source": [
    "class PolicyApproximator(torch.nn.Module):\n",
    "    def __init__(self, state_dim: int, num_actions: int, hidden_sizes: list[int],\n",
    "                 activation: str,\n",
    "                 output_activation: str,\n",
    "                 learning_rate: float):\n",
    "        \"\"\"\n",
    "        Neural network that approximates the policy:\n",
    "            - takes in a (batch of) observation(s)\n",
    "            - outputs a (batch of) action probabilities (i.e., probability of taking each action in the batch)\n",
    "\n",
    "\n",
    "\n",
    "        obs_dim: the dimension of the observation space\n",
    "        num_actions: the cardinality of the action space.\n",
    "        hidden_sizes: the hidden sizes of the network\n",
    "        activation: activation functions of the network\n",
    "        output_activation: output activation function of the network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = build_mlp_network(\n",
    "            sizes=[state_dim, *hidden_sizes, num_actions],\n",
    "            activation=activation,\n",
    "            output_activation=output_activation,\n",
    "        )\n",
    "\n",
    "        self.lr = learning_rate\n",
    "\n",
    "        # Set up the optimizer. By default its Adam, but you can experiment with this        \n",
    "        self.optim = torch.optim.Adam(params=self.model.parameters(),\n",
    "                                      lr=learning_rate)\n",
    "\n",
    "    def get_probabilities(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        # ============ YOUR CODE HERE =============\n",
    "        # implement a method that, given the (batch of) observations 'obs', of\n",
    "        # shape (n, O), outputs a tensor of shape (n, A) with the probabilities\n",
    "        # of taking each action.\n",
    "        # ============ YOUR CODE HERE =============\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def get_log_probs(self, s, a):\n",
    "        # ============ YOUR CODE HERE =============\n",
    "        # get the log-probability of given observation-action pairs.\n",
    "        # ============ YOUR CODE HERE =============    \n",
    "        pass\n",
    "\n",
    "    def update(self, obs: torch.Tensor, target: torch.Tensor, action: torch.Tensor) -> None:\n",
    "        \n",
    "        # Given an (observation, action) pair and a 'target' for REINFORCE, this method:\n",
    "        # 1. Computes the loss (you need to implement this)\n",
    "        # 2. gets the gradients w.r.t. the policy parameters\n",
    "        # 3. takes a step of gradient descent.\n",
    "        # In practice, obs, target, and action can be a given (s_t, a_t, G_t) tuple, where s_t and a_t are the state and action\n",
    "        # at time t, and G_t is the (discounted) return from time t onwards.\n",
    "        \n",
    "        # zero out the optimizer\n",
    "        self.optim.zero_grad()\n",
    "\n",
    "        # the action is a numpy array with value either 0, 1, or 2.\n",
    "        # Compute the loss. \n",
    "        # ============ YOUR CODE HERE =============\n",
    "        # loss = ...\n",
    "        # ============ YOUR CODE HERE =============\n",
    "\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "        loss.backward()        \n",
    "        self.optim.step()\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 [Coding task, 20 points]\n",
    "Complete the highlighted methods for the ReinforceAgent class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRM03FjTMPCm"
   },
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, env: gym.Env, gamma: float, policy_params: dict):\n",
    "\n",
    "        # Initial observation\n",
    "        state = env.reset()\n",
    "        \n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.pi = PolicyApproximator(state_dim=state_dim,\n",
    "                                     num_actions=num_actions,\n",
    "                                     hidden_sizes=policy_params['hidden_sizes'],\n",
    "                                     activation=policy_params['activation'],\n",
    "                                     output_activation=policy_params['output_activation'],\n",
    "                                     learning_rate=policy_params['learning_rate'],\n",
    "                                    )\n",
    "        # internal variables\n",
    "        self._state = state\n",
    "        self._action = 0\n",
    "        \n",
    "\n",
    "    def step(self, state): \n",
    "        # ============ YOUR CODE HERE =============\n",
    "        # Get the action probabilities\n",
    "        # action_probs =\n",
    "    \n",
    "        # Select you action\n",
    "        # action =\n",
    "    \n",
    "        # Update the internal variables\n",
    "        # ============ YOUR CODE HERE =============\n",
    "\n",
    "        \n",
    "\n",
    "        # Update internal variables\n",
    "        self._action = action\n",
    "        self._state = state\n",
    "        return self._action\n",
    "\n",
    "    def update(self, episode: list) -> None:\n",
    "        \"\"\"\n",
    "        Given information of an episode,\n",
    "        updates the policy based on reinforce\n",
    "\n",
    "        Args:\n",
    "            episode (list): a list containing (s,a,r,s') transitions.\n",
    "        \"\"\"\n",
    "        # ============ YOUR CODE HERE =============\n",
    "        # For each s,a pair in the episode, compute:\n",
    "        # the discounted return (G), update the policy estimator based on the return.\n",
    "        # s = ...\n",
    "        # a = ...\n",
    "        # G = ...\n",
    "        # ============ YOUR CODE HERE =============\n",
    "\n",
    "        \n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiment!\n",
    "- Let's try it out!\n",
    "- You can play around with the policy paremeters in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title [IMPORTANT] Run REINFORCE agent with an environment 'env'\n",
    "# Description:\n",
    "# Simple experiment run loop\n",
    "# -----------------------------------------------------------------------------\n",
    "# Expected behaviour\n",
    "# 1) For each episode repeat:\n",
    "#  - Interact with the environment (get observation and discount)\n",
    "#  - Store transition\n",
    "# 2) At the end of the episode, use the stored transition to update agent\n",
    "# Repeat for num_episode\n",
    "# -----------------------------------------------------------------------------\n",
    "# Additional: Log and return episode stastics for plotting later on\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_reinforce(env, agent, num_episodes,\n",
    "                  MAXSTEPS_PER_EPISODE=100,\n",
    "                  REPORT_EVERY_N_STEPS=20):\n",
    "    \"\"\"\n",
    "    Run REINFORCE agent in a MDP especified by 'env'.\n",
    "    (Any agent that follows the same logic and can be plugged in though.)\n",
    "\n",
    "    Agent requirements:\n",
    "      agent.step(state)\n",
    "      agent.update(episode)\n",
    "\n",
    "    Enviroment requirements:\n",
    "      env.step(action)\n",
    "\n",
    "    -----------------------------------------------------------------------\n",
    "    Inputs:\n",
    "        env: mountain car\n",
    "        agent: REINFORCE agent (or alternative)\n",
    "        num_episodes: Number of episodes to run for\n",
    "\n",
    "    Returns:\n",
    "        episode_statistics: list containing (episode_length & episode_reward) for each episode.\n",
    "    \"\"\"\n",
    "\n",
    "    # Book-keeping of some useful (episode) statistics\n",
    "    stats = Logging_EpStats(\n",
    "    episode_lengths=np.zeros(num_episodes),\n",
    "    episode_rewards=np.zeros(num_episodes))\n",
    "\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "\n",
    "        # Reset the environment and pick the first action\n",
    "        action = agent.step(torch.Tensor(state))\n",
    "        # print(f'sampled action {action}')\n",
    "        # print(f'action has type {type(action)}')\n",
    "        next_s, r, term, trunc, _ = env.step(int(action))\n",
    "        # print(f'r is {r}')\n",
    "\n",
    "        episode = []\n",
    "\n",
    "        # One step in the environment\n",
    "        state = next_s\n",
    "        for t in range(MAXSTEPS_PER_EPISODE):\n",
    "\n",
    "            # Take a step\n",
    "            action = agent.step(torch.Tensor(state))\n",
    "            # print(f'sampled action {action}')\n",
    "            next_s, r, term, trunc, _ = env.step(int(action))\n",
    "\n",
    "            # Keep track of the transition\n",
    "            episode.append((state, action, r, next_s))\n",
    "\n",
    "            # Optional: Logging and reporting (live) statistics for this epsiode\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += r\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            done = term or trunc\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_s\n",
    "\n",
    "        # Make the policy update\n",
    "        agent.update(episode)        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "gamma = 0.99\n",
    "\n",
    "policy_params = dict(\n",
    "    hidden_sizes=[32],\n",
    "    activation='relu',\n",
    "    output_activation='softmax',\n",
    "    learning_rate = 5e-4,\n",
    ")\n",
    "\n",
    "num_episodes = 1000\n",
    "agent = REINFORCEAgent(env, gamma, policy_params)\n",
    "stats = run_reinforce(env, agent, num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stats(stats, window=10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
